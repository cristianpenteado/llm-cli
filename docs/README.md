# LLM-CLI - Assistente de IA para Desenvolvimento

[![Version](https://img.shields.io/badge/version-2.0.0-blue.svg)](CHANGELOG.md)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Node.js](https://img.shields.io/badge/node-%3E%3D18.0.0-brightgreen.svg)](package.json)
[![TypeScript](https://img.shields.io/badge/typescript-5.3.2-blue.svg)](package.json)

> **CLI inteligente para desenvolvimento com modelos LLMs locais via Ollama**

## ğŸš€ VisÃ£o Geral

O LLM-CLI Ã© uma ferramenta de linha de comando que transforma a forma como vocÃª desenvolve software. Em vez de comandos explÃ­citos, ele funciona como um **chat conversacional inteligente** que entende suas intenÃ§Ãµes e executa aÃ§Ãµes automaticamente.

### âœ¨ CaracterÃ­sticas Principais

- **ğŸ¤– Chat Natural**: Converse como se estivesse falando com um colega desenvolvedor
- **ğŸ¯ DetecÃ§Ã£o AutomÃ¡tica**: Identifica quando vocÃª quer implementar algo
- **ğŸ“‹ Planejamento Inteligente**: Cria planos detalhados automaticamente
- **âš¡ ExecuÃ§Ã£o Guiada**: Executa passos com confirmaÃ§Ãµes inteligentes
- **ğŸ’¡ SugestÃµes de Comandos**: Recomenda e executa comandos do sistema
- **ğŸ—ï¸ Arquitetura DDD**: CÃ³digo limpo e bem estruturado

## ğŸ¯ Casos de Uso

### ğŸ’¬ **Perguntas Conceituais**
```
> Como funciona JWT?
> Qual a diferenÃ§a entre REST e GraphQL?
> Explique o padrÃ£o Repository
```

### ğŸ› ï¸ **ImplementaÃ§Ã£o AutomÃ¡tica**
```
> Quero criar uma API REST com Express
> Implementa autenticaÃ§Ã£o no meu projeto
> Configurar TypeScript no projeto
```

### âš¡ **Comandos Sugeridos**
```
> Configurar projeto Node.js
ğŸ’¡ Comandos sugeridos:
  â€¢ npm init -y
  â€¢ npm install express
  â€¢ npm install -D typescript @types/node
```

## ğŸš€ InstalaÃ§Ã£o

### PrÃ©-requisitos
- **Node.js** >= 18.0.0
- **Ollama** instalado e rodando
- **Modelo LLM** baixado (ex: `llama3.2`)

### InstalaÃ§Ã£o Global
```bash
# Clone o repositÃ³rio
git clone https://github.com/llm-cli/llm-cli.git
cd llm-cli

# Instale dependÃªncias e compile
npm install
npm run build

# Instale globalmente
npm install -g .
```

### VerificaÃ§Ã£o
```bash
llm --version
# SaÃ­da: 1.0.0
```

## ğŸ® Como Usar

### Iniciar Chat
```bash
llm
```

### Uso BÃ¡sico
```bash
# Ajuda
llm --help

# Modelo especÃ­fico
llm --model llama3.2
```

### Exemplo de Conversa
```
ğŸ¤– LLM-CLI iniciado com modelo llama3.2

> Quero criar uma API REST com Express

ğŸ’¬ Resposta: Vou criar uma API REST completa com Express para vocÃª...

ğŸ¯ Detectei uma solicitaÃ§Ã£o de implementaÃ§Ã£o!
â“ Quer que eu crie um plano detalhado para isso? [s/n]: s

âœ… Plano criado: API REST com Express
ğŸ“ Passos:
  â–¶ï¸ 1. Setup inicial do projeto
     Criar package.json e estrutura bÃ¡sica
  â¸ï¸ 2. Instalar Express e dependÃªncias
     npm install express cors helmet
  â¸ï¸ 3. Criar servidor bÃ¡sico
     Arquivo server.js com rotas essenciais

â“ Executar o plano agora? [s/n]: s

ğŸš€ Executando plano...
ğŸ“ Passo 1: Setup inicial do projeto
Executar este passo? [sim/skip/stop]: sim
âœ… Passo concluÃ­do
```

## ğŸ—ï¸ Arquitetura

### Estrutura do Projeto
```
src/
â”œâ”€â”€ domain/           # Regras de negÃ³cio
â”‚   â”œâ”€â”€ agent/       # DomÃ­nio do agente IA
â”‚   â”œâ”€â”€ communication/ # ComunicaÃ§Ã£o com modelos
â”‚   â””â”€â”€ configuration/ # ConfiguraÃ§Ã£o
â”œâ”€â”€ application/     # ServiÃ§os de aplicaÃ§Ã£o
â”‚   â”œâ”€â”€ services/   # AgentService
â”‚   â””â”€â”€ ports/      # Interfaces
â””â”€â”€ infrastructure/ # ImplementaÃ§Ãµes
    â”œâ”€â”€ cli/       # Interface conversacional
    â”œâ”€â”€ ollama/    # IntegraÃ§Ã£o direta
    â”œâ”€â”€ filesystem/
    â”œâ”€â”€ logging/
    â”œâ”€â”€ configuration/
    â””â”€â”€ di/        # InjeÃ§Ã£o de dependÃªncia
```

### Camadas
- **Domain**: Entidades e regras de negÃ³cio
- **Application**: ServiÃ§os e casos de uso
- **Infrastructure**: ImplementaÃ§Ãµes concretas

## âš™ï¸ ConfiguraÃ§Ã£o

### Arquivo de ConfiguraÃ§Ã£o
Crie `~/.llm-cli/config.yaml`:

```yaml
model:
  defaultModel: "llama3.2"
  temperature: 0.7
  maxTokens: 4096
  fallbackModels: ["llama3.2:1b", "qwen2.5:7b"]

agent:
  name: "Claude Code Assistant"
  personality: "helpful"  # helpful|concise|detailed|creative
  autoConfirm: false
  maxPlanSteps: 10
  contextWindow: 8192

cli:
  theme: "auto"  # dark|light|auto
  showTimestamps: false
  logLevel: "info"  # error|warn|info|debug
  historySize: 100

ollama:
  host: "localhost"
  port: 11434
  timeout: 30000
  retryAttempts: 3
  keepAlive: "5m"
```

### Personalidades do Agente
- **helpful**: AmigÃ¡vel e prestativo (padrÃ£o)
- **concise**: Respostas mais diretas
- **detailed**: ExplicaÃ§Ãµes mais profundas
- **creative**: SoluÃ§Ãµes mais inovadoras

## ğŸ§ª Testes

### Executar Testes
```bash
# Todos os testes
npm test

# Testes em modo watch
npm run test:watch

# Build
npm run build
```

### Cobertura
- **Unit Tests**: AgentService, OllamaProvider
- **Integration Tests**: Fluxo completo CLI
- **Status**: 14/14 testes passando âœ…

## ğŸ”§ Desenvolvimento

### Scripts DisponÃ­veis
```bash
npm run build      # Compilar TypeScript
npm run dev        # Executar em modo desenvolvimento
npm run start      # Executar versÃ£o compilada
npm run test       # Executar testes
npm run lint       # Verificar cÃ³digo
npm run clean      # Limpar dist/
```

### Estrutura de Desenvolvimento
```
src/
â”œâ”€â”€ __tests__/     # Testes
â”œâ”€â”€ application/   # ServiÃ§os de aplicaÃ§Ã£o
â”œâ”€â”€ domain/        # Regras de negÃ³cio
â””â”€â”€ infrastructure/ # ImplementaÃ§Ãµes
```

## ğŸ“š DocumentaÃ§Ã£o

- **[Arquitetura](ARCHITECTURE.md)**: Detalhes tÃ©cnicos e decisÃµes
- **[Guia Conversacional](CONVERSATIONAL-GUIDE.md)**: Como usar o chat
- **[Changelog](CHANGELOG.md)**: HistÃ³rico de versÃµes

## ğŸ¤ ContribuiÃ§Ã£o

### Como Contribuir
1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

### PadrÃµes de CÃ³digo
- **TypeScript**: Tipagem forte e interfaces bem definidas
- **DDD**: Domain-Driven Design para organizaÃ§Ã£o
- **Clean Code**: CÃ³digo limpo e legÃ­vel
- **Testes**: Cobertura de testes para todas as funcionalidades

## ğŸ› SoluÃ§Ã£o de Problemas

### Problemas Comuns

#### **Erro: "Service ConversationContext not found"**
```bash
# Recompile o projeto
npm run build
npm install -g .
```

#### **Ollama nÃ£o responde**
```bash
# Verifique se Ollama estÃ¡ rodando
ollama list

# Teste a conexÃ£o
curl http://localhost:11434/api/tags
```

#### **Modelo nÃ£o encontrado**
```bash
# Liste modelos disponÃ­veis
ollama list

# Baixe um modelo
ollama pull llama3.2
```

### Logs e Debug
```bash
# Aumente nÃ­vel de log
llm --log-level debug

# Verifique logs do sistema
tail -f ~/.llm-cli/logs/app.log
```

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ™ Agradecimentos

- **Ollama** pela plataforma de modelos locais
- **Comunidade TypeScript** pelas ferramentas e padrÃµes
- **Contribuidores** que ajudaram a construir esta ferramenta

## ğŸ“ Suporte

- **Issues**: [GitHub Issues](https://github.com/llm-cli/llm-cli/issues)
- **Discussions**: [GitHub Discussions](https://github.com/llm-cli/llm-cli/discussions)
- **DocumentaÃ§Ã£o**: [docs/](docs/)

---

**Desenvolvido com â¤ï¸ pela equipe LLM-CLI**
