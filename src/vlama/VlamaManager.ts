import { exec } from 'child_process';
import { promisify } from 'util';
import { VlamaConfig, ModelConfig } from '../types';
import { Logger } from '../utils/Logger';
import { ConfigManager } from '../utils/ConfigManager';
import * as fs from 'fs-extra';
import * as path from 'path';

const execAsync = promisify(exec);

export class VlamaManager {
  private config: VlamaConfig;
  private configManager: ConfigManager;
  private isConnected: boolean = false;
  private serverProcess: any = null;

  constructor() {
    this.configManager = new ConfigManager();
    this.config = this.configManager.getVlamaConfig();
  }

  /**
   * Inicializa o gerenciador Vlama
   */
  async initialize(): Promise<void> {
    try {
      Logger.vlama('üöÄ Inicializando gerenciador Vlama...');
      
      // Verificar se o Vlama est√° instalado
      await this.checkVlamaInstallation();
      
      // Iniciar servidor Vlama se necess√°rio
      await this.startVlamaServer();
      
      // Conectar ao servidor
      await this.connect();
      
      Logger.success('‚úÖ Gerenciador Vlama inicializado');
      
    } catch (error) {
      Logger.error('Erro ao inicializar gerenciador Vlama:', error);
      throw error;
    }
  }

  /**
   * Verifica se o Vlama est√° instalado
   */
  private async checkVlamaInstallation(): Promise<void> {
    try {
      const { stdout } = await execAsync('vlama --version');
      Logger.vlama(`‚úÖ Vlama instalado: ${stdout.trim()}`);
    } catch (error) {
      Logger.warn('Vlama n√£o encontrado, tentando instalar...');
      await this.installVlama();
    }
  }

  /**
   * Instala o Vlama
   */
  private async installVlama(): Promise<void> {
    try {
      Logger.vlama('üì¶ Instalando Vlama...');
      
      // Tentar diferentes m√©todos de instala√ß√£o
      try {
        await execAsync('curl -fsSL https://get.vlama.ai | sh');
      } catch {
        try {
          await execAsync('wget -qO- https://get.vlama.ai | sh');
        } catch {
          throw new Error('N√£o foi poss√≠vel instalar o Vlama automaticamente');
        }
      }
      
      Logger.success('‚úÖ Vlama instalado com sucesso');
      
    } catch (error) {
      Logger.error('Erro ao instalar Vlama:', error);
      throw new Error('Falha ao instalar Vlama. Instale manualmente: https://vlama.ai');
    }
  }

  /**
   * Inicia o servidor Vlama
   */
  private async startVlamaServer(): Promise<void> {
    try {
      // Verificar se o servidor j√° est√° rodando
      if (await this.isServerRunning()) {
        Logger.vlama('‚úÖ Servidor Vlama j√° est√° rodando');
        return;
      }

      Logger.vlama('üöÄ Iniciando servidor Vlama...');
      
      // Iniciar servidor em background
      this.serverProcess = exec('vlama serve', {
        env: {
          ...process.env,
          VLAMA_MODELS_PATH: this.config.modelsPath,
          VLAMA_MAX_MEMORY: this.config.maxMemory.toString(),
          VLAMA_GPU_ENABLED: this.config.gpuEnabled.toString()
        }
      });

      // Aguardar servidor estar pronto
      await this.waitForServerReady();
      
      Logger.success('‚úÖ Servidor Vlama iniciado');
      
    } catch (error) {
      Logger.error('Erro ao iniciar servidor Vlama:', error);
      throw error;
    }
  }

  /**
   * Verifica se o servidor est√° rodando
   */
  private async isServerRunning(): Promise<boolean> {
    try {
      const { stdout } = await execAsync(`curl -s http://${this.config.serverUrl}:${this.config.port}/health`);
      return stdout.includes('ok') || stdout.includes('healthy');
    } catch {
      return false;
    }
  }

  /**
   * Aguarda o servidor estar pronto
   */
  private async waitForServerReady(): Promise<void> {
    let attempts = 0;
    const maxAttempts = 30;
    
    while (attempts < maxAttempts) {
      try {
        if (await this.isServerRunning()) {
          return;
        }
        await new Promise(resolve => setTimeout(resolve, 1000));
        attempts++;
      } catch {
        attempts++;
      }
    }
    
    throw new Error('Timeout aguardando servidor Vlama');
  }

  /**
   * Conecta ao servidor Vlama
   */
  async connect(): Promise<void> {
    try {
      Logger.vlama('üîå Conectando ao servidor Vlama...');
      
      // Testar conex√£o usando curl
      const { stdout } = await execAsync(`curl -s http://${this.config.serverUrl}:${this.config.port}/health`);
      
      if (stdout.includes('ok') || stdout.includes('healthy')) {
        this.isConnected = true;
        Logger.success('‚úÖ Conectado ao servidor Vlama');
      } else {
        throw new Error('Servidor Vlama n√£o respondeu corretamente');
      }
      
    } catch (error) {
      Logger.error('Erro ao conectar ao servidor Vlama:', error);
      this.isConnected = false;
      throw new Error('Falha ao conectar ao servidor Vlama');
    }
  }

  /**
   * Desconecta do servidor Vlama
   */
  async disconnect(): Promise<void> {
    if (this.isConnected) {
      try {
        Logger.vlama('üîå Desconectando do servidor Vlama...');
        
        await execAsync(`curl -s -X POST http://${this.config.serverUrl}:${this.config.port}/shutdown`);
        
        this.isConnected = false;
        Logger.success('‚úÖ Desconectado do servidor Vlama');
        
      } catch (error) {
        Logger.warn('Erro ao desconectar do servidor Vlama:', error);
      }
    }
  }

  /**
   * Verifica se est√° conectado
   */
  isConnectedToServer(): boolean {
    return this.isConnected;
  }

  /**
   * Lista modelos dispon√≠veis
   */
  async listModels(): Promise<ModelConfig[]> {
    if (!this.isConnected) {
      await this.connect();
    }

    try {
      Logger.vlama('üìã Listando modelos dispon√≠veis...');
      
      const { stdout } = await execAsync(`curl -s http://${this.config.serverUrl}:${this.config.port}/models`);
      const models = JSON.parse(stdout);
      
      Logger.vlama(`‚úÖ ${models.length} modelos encontrados`);
      return models;
      
    } catch (error) {
      Logger.error('Erro ao listar modelos:', error);
      return [];
    }
  }

  /**
   * Obt√©m informa√ß√µes de um modelo espec√≠fico
   */
  async getModelInfo(modelName: string): Promise<ModelConfig | null> {
    if (!this.isConnected) {
      await this.connect();
    }

    try {
      Logger.vlama(`‚ÑπÔ∏è Obtendo informa√ß√µes do modelo ${modelName}...`);
      
      const { stdout } = await execAsync(`curl -s http://${this.config.serverUrl}:${this.config.port}/models/${encodeURIComponent(modelName)}`);
      const modelInfo = JSON.parse(stdout);
      
      Logger.vlama('‚úÖ Informa√ß√µes do modelo obtidas');
      return modelInfo;
      
    } catch (error) {
      Logger.error('Erro ao obter informa√ß√µes do modelo:', error);
      return null;
    }
  }

  /**
   * Inicia um modelo
   */
  async startModel(modelName: string): Promise<void> {
    if (!this.isConnected) {
      await this.connect();
    }

    try {
      Logger.vlama(`üöÄ Iniciando modelo ${modelName}...`);
      
      await execAsync(`curl -s -X POST http://${this.config.serverUrl}:${this.config.port}/models/${encodeURIComponent(modelName)}/start`);
      
      Logger.vlama(`‚úÖ Modelo ${modelName} iniciado`);
      
    } catch (error) {
      Logger.error('Erro ao iniciar modelo:', error);
      throw error;
    }
  }

  /**
   * Para um modelo
   */
  async stopModel(modelName: string): Promise<void> {
    if (!this.isConnected) {
      await this.connect();
    }

    try {
      Logger.vlama(`üõë Parando modelo ${modelName}...`);
      
      await execAsync(`curl -s -X POST http://${this.config.serverUrl}:${this.config.port}/models/${encodeURIComponent(modelName)}/stop`);
      
      Logger.vlama(`‚úÖ Modelo ${modelName} parado`);
      
    } catch (error) {
      Logger.error('Erro ao parar modelo:', error);
      throw error;
    }
  }

  /**
   * Obt√©m status de um modelo
   */
  async getModelStatus(modelName: string): Promise<string> {
    if (!this.isConnected) {
      await this.connect();
    }

    try {
      Logger.vlama(`üìä Verificando status do modelo ${modelName}...`);
      
      const { stdout } = await execAsync(`curl -s http://${this.config.serverUrl}:${this.config.port}/models/${encodeURIComponent(modelName)}/status`);
      const status = JSON.parse(stdout);
      
      Logger.vlama(`‚úÖ Status do modelo ${modelName}: ${status.status}`);
      return status.status;
      
    } catch (error) {
      Logger.error('Erro ao obter status do modelo:', error);
      return 'error';
    }
  }

  /**
   * Faz download de um modelo
   */
  async downloadModel(modelName: string): Promise<void> {
    if (!this.isConnected) {
      await this.connect();
    }

    try {
      Logger.vlama(`üì• Fazendo download do modelo ${modelName}...`);
      
      await execAsync(`curl -s -X POST http://${this.config.serverUrl}:${this.config.port}/models/${encodeURIComponent(modelName)}/download`);
      
      Logger.vlama(`‚úÖ Download do modelo ${modelName} conclu√≠do`);
      
    } catch (error) {
      Logger.error('Erro ao fazer download do modelo:', error);
      throw error;
    }
  }

  /**
   * Remove um modelo
   */
  async removeModel(modelName: string): Promise<void> {
    if (!this.isConnected) {
      await this.connect();
    }

    try {
      Logger.vlama(`üóëÔ∏è Removendo modelo ${modelName}...`);
      
      await execAsync(`curl -s -X DELETE http://${this.config.serverUrl}:${this.config.port}/models/${encodeURIComponent(modelName)}`);
      
      Logger.vlama(`‚úÖ Modelo ${modelName} removido`);
      
    } catch (error) {
      Logger.error('Erro ao remover modelo:', error);
      throw error;
    }
  }

  /**
   * Obt√©m estat√≠sticas do servidor
   */
  async getServerStats(): Promise<any> {
    if (!this.isConnected) {
      await this.connect();
    }

    try {
      Logger.vlama('üìä Obtendo estat√≠sticas do servidor...');
      
      const { stdout } = await execAsync(`curl -s http://${this.config.serverUrl}:${this.config.port}/stats`);
      const stats = JSON.parse(stdout);
      
      Logger.vlama('‚úÖ Estat√≠sticas do servidor obtidas');
      return stats;
      
    } catch (error) {
      Logger.error('Erro ao obter estat√≠sticas do servidor:', error);
      return null;
    }
  }

  /**
   * Para o servidor Vlama
   */
  async shutdown(): Promise<void> {
    try {
      Logger.vlama('üõë Parando servidor Vlama...');
      
      if (this.serverProcess) {
        this.serverProcess.kill();
        this.serverProcess = null;
      }
      
      await this.disconnect();
      
      Logger.success('‚úÖ Servidor Vlama parado');
      
    } catch (error) {
      Logger.error('Erro ao parar servidor Vlama:', error);
      throw error;
    }
  }

  /**
   * Formata tamanho do modelo para exibi√ß√£o
   */
  private formatModelSize(bytes: number): string {
    const sizes = ['B', 'KB', 'MB', 'GB', 'TB'];
    if (bytes === 0) return '0 B';
    
    const i = Math.floor(Math.log(bytes) / Math.log(1024));
    return Math.round(bytes / Math.pow(1024, i) * 100) / 100 + ' ' + sizes[i];
  }

  /**
   * Verifica se o servidor est√° rodando e pronto
   */
  async isServerReady(): Promise<boolean> {
    try {
      return await this.isServerRunning();
    } catch {
      return false;
    }
  }

  /**
   * Obt√©m estat√≠sticas de uso
   */
  getVlamaStats(): any {
    return {
      isConnected: this.isConnected,
      serverUrl: this.config.serverUrl,
      port: this.config.port,
      modelsPath: this.config.modelsPath,
      maxMemory: this.config.maxMemory,
      gpuEnabled: this.config.gpuEnabled,
      hasServerProcess: !!this.serverProcess
    };
  }
}
