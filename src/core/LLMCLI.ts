import { ProjectManager } from './ProjectManager';
import { ModelManager } from './ModelManager';
import { ConversationManager } from './ConversationManager';
import { FileManager } from './FileManager';
import { MCPClient } from '../mcp/MCPClient';
import { OllamaManager } from '../ollama/OllamaManager';
import { Logger } from '../utils/Logger';
import { ConfigManager } from '../utils/ConfigManager';
import { ProjectConfig, ModelConfig } from '../types';

export class LLMCLI {
  private projectManager: ProjectManager;
  private modelManager: ModelManager;
  private conversationManager: ConversationManager;
  private fileManager: FileManager;
  private mcpClient: MCPClient;
  private ollamaManager: OllamaManager;
  private configManager: ConfigManager;
  private currentProject: ProjectConfig | null = null;
  private verboseLogs: boolean;

  constructor(verboseLogs: boolean = false) {
    this.verboseLogs = verboseLogs;
    this.configManager = new ConfigManager();
    this.ollamaManager = new OllamaManager(verboseLogs);
    this.mcpClient = new MCPClient(this.ollamaManager, verboseLogs);
    this.projectManager = new ProjectManager();
    this.modelManager = new ModelManager(this.ollamaManager, this.mcpClient, verboseLogs);
    this.conversationManager = new ConversationManager(this.modelManager, verboseLogs);
    this.fileManager = new FileManager();
  }

  /**
   * Inicializa um novo projeto na pasta atual
   */
  async initializeProject(options: { model?: string; force?: boolean }): Promise<void> {
    try {
      Logger.info('üöÄ Inicializando novo projeto...');

      // Validar estado do sistema antes de continuar
      await this.validateSystemState();

      // Inicializar OllamaManager (baixa modelo padr√£o automaticamente)
      await this.ollamaManager.initialize();

      // Inicializar projeto
      const projectConfig = await this.projectManager.initializeProject(options);
      this.currentProject = projectConfig;

      // Configurar modelo
      let selectedModel = options.model;
      
      if (!selectedModel) {
        const defaultModel = await this.configManager.getDefaultModel();
        if (defaultModel) {
          // Verificar se o modelo padr√£o est√° dispon√≠vel
          selectedModel = await this.modelManager.ensureModelReady(defaultModel);
        } else {
          // Nenhum modelo padr√£o, usar sele√ß√£o interativa
          Logger.info('ü§ñ Nenhum modelo padr√£o configurado. Vamos escolher um!');
          selectedModel = await this.modelManager.selectModel();
        }
      } else {
        // Verificar se o modelo especificado est√° dispon√≠vel
        selectedModel = await this.modelManager.ensureModelReady(selectedModel);
      }

      // Configurar modelo no projeto
      await this.modelManager.setProjectModel(projectConfig.path, selectedModel);
      this.currentProject.model = selectedModel;

      // Salvar modelo na configura√ß√£o do projeto
      await this.projectManager.updateProjectModel(projectConfig.path, selectedModel);

      // Definir como modelo padr√£o se for primeira execu√ß√£o
      if (await this.configManager.isFirstRun()) {
        await this.configManager.setDefaultModel(selectedModel);
        Logger.info(`üíæ Modelo ${selectedModel} definido como padr√£o global`);
      }

      Logger.success(`‚úÖ Projeto inicializado em: ${projectConfig.path}`);
      Logger.info(`üìÅ Estrutura do projeto: ${projectConfig.language}/${projectConfig.framework}`);
      Logger.info(`ü§ñ Modelo configurado: ${selectedModel}`);
      
      // Mostrar pr√≥ximos passos
      Logger.newline();
      Logger.info('üéØ Pr√≥ximos passos:');
      Logger.info('  1. Use "llm chat" para iniciar uma conversa com a IA');
      Logger.info('  2. Use "llm status" para ver o status do projeto');
      Logger.info('  3. Use "llm change-model" para trocar o modelo se necess√°rio');
      
    } catch (error) {
      Logger.error('Erro ao inicializar projeto:', error);
      throw error;
    }
  }

  /**
   * Valida o estado do sistema antes de executar comandos
   */
  private async validateSystemState(): Promise<void> {
    try {
      Logger.info('üîç Validando estado do sistema...');
      
      // Verificar se o Ollama est√° instalado
      try {
        const { exec } = await import('child_process');
        const { promisify } = await import('util');
        const execAsync = promisify(exec);
        
        await execAsync('ollama --version');
        Logger.info('‚úÖ Ollama est√° instalado');
      } catch (error) {
        throw new Error('Ollama n√£o est√° instalado. Instale em: https://ollama.ai');
      }
      
      // Verificar se o servidor Ollama est√° rodando
      try {
        const { exec } = await import('child_process');
        const { promisify } = await import('util');
        const execAsync = promisify(exec);
        
        await execAsync('ollama list', { timeout: 10000 });
        Logger.info('‚úÖ Servidor Ollama est√° ativo');
      } catch (error) {
        throw new Error('Servidor Ollama n√£o est√° respondendo. Execute: ollama serve');
      }
      
      Logger.info('‚úÖ Sistema validado com sucesso');
      
    } catch (error) {
      Logger.error('‚ùå Falha na valida√ß√£o do sistema:', error);
      throw error;
    }
  }

  /**
   * Troca o modelo LLM para o projeto atual
   */
  async changeModel(modelName?: string): Promise<void> {
    try {
      // Verificar se h√° um projeto ativo
      if (!this.currentProject) {
        // Tentar carregar projeto da pasta atual
        const currentPath = process.cwd();
        const projectConfig = await this.projectManager.loadProject(currentPath);
        
        if (projectConfig) {
          this.currentProject = projectConfig;
          Logger.info(`üìÅ Projeto carregado: ${projectConfig.name}`);
        } else {
          throw new Error('Nenhum projeto ativo. Execute "llm init" primeiro.');
        }
      }

      if (!modelName) {
        // Usar sele√ß√£o interativa
        Logger.info('üîÑ Vamos escolher um novo modelo para o projeto!');
        modelName = await this.modelManager.selectModel();
      }

      Logger.info(`üîÑ Alterando modelo para: ${modelName}`);

      // Verificar se o modelo est√° dispon√≠vel
      const availableModels = await this.ollamaManager.listModels();
      const modelExists = availableModels.some(m => m.name === modelName);

      if (!modelExists) {
        Logger.warn(`‚ö†Ô∏è Modelo "${modelName}" n√£o encontrado. Tentando baixar...`);
        try {
          await this.ollamaManager.downloadModelWithProgress(modelName);
          Logger.success(`‚úÖ Modelo ${modelName} baixado com sucesso!`);
        } catch (downloadError) {
          throw new Error(`Modelo "${modelName}" n√£o encontrado e n√£o p√¥de ser baixado. Use "ollama pull ${modelName}" manualmente.`);
        }
      }

      // Parar processo persistente anterior
      await this.ollamaManager.stopModelSession();

      // Configurar novo modelo
      await this.modelManager.setProjectModel(this.currentProject.path, modelName);
      this.currentProject.model = modelName;

      // Salvar na configura√ß√£o do projeto
      await this.projectManager.updateProjectModel(this.currentProject.path, modelName);

      // Configurar novo modelo (sem aguardar inicializa√ß√£o completa)
      Logger.info(`üöÄ Iniciando modelo ${modelName} em background...`);
      
      // Iniciar modelo em background de forma n√£o-bloqueante
      this.ollamaManager.initialize(modelName).catch((error) => {
        Logger.warn(`‚ö†Ô∏è Erro ao inicializar modelo em background: ${error}`);
      });

      Logger.success(`‚úÖ Modelo alterado para: ${modelName}`);
      Logger.info(`üí° Use "llm chat" para iniciar uma conversa com o novo modelo`);
      Logger.info(`üîÑ Modelo est√° sendo inicializado em background...`);
      
    } catch (error) {
      Logger.error('Erro ao trocar modelo:', error);
      throw error;
    }
  }

  /**
   * Mostra o status atual do projeto e modelo
   */
  async showStatus(): Promise<void> {
    try {
      Logger.info('üìä Status do Projeto LLM');
      Logger.info('‚ïê'.repeat(50));
      
      // Verificar se h√° um projeto ativo
      if (!this.currentProject) {
        const currentPath = process.cwd();
        const projectConfig = await this.projectManager.loadProject(currentPath);
        
        if (projectConfig) {
          this.currentProject = projectConfig;
        } else {
          Logger.warn('‚ö†Ô∏è Nenhum projeto ativo nesta pasta');
          Logger.info('üí° Execute "llm init" para inicializar um projeto');
          return;
        }
      }
      
      // Informa√ß√µes do projeto
      Logger.info(`üìÅ Projeto: ${this.currentProject.name}`);
      Logger.info(`üìç Caminho: ${this.currentProject.path}`);
      Logger.info(`üîß Linguagem: ${this.currentProject.language}`);
      Logger.info(`üèóÔ∏è Framework: ${this.currentProject.framework}`);
      Logger.info(`ü§ñ Modelo: ${this.currentProject.model || 'N√£o configurado'}`);
      Logger.info(`üìÖ Criado em: ${this.currentProject.createdAt.toLocaleDateString('pt-BR')}`);
      
      if (this.currentProject.updatedAt) {
        Logger.info(`üìÖ Atualizado em: ${this.currentProject.updatedAt.toLocaleDateString('pt-BR')}`);
      }
      
      Logger.newline();
      
      // Status do Ollama
      try {
        const { exec } = await import('child_process');
        const { promisify } = await import('util');
        const execAsync = promisify(exec);
        
        const { stdout: version } = await execAsync('ollama --version');
        Logger.info(`üöÄ Ollama: ${version.trim()}`);
        
        const { stdout: models } = await execAsync('ollama list');
        const modelCount = models.trim().split('\n').length - 1; // -1 para cabe√ßalho
        Logger.info(`üìã Modelos dispon√≠veis: ${modelCount}`);
        
        // Mostrar modelos dispon√≠veis
        const modelLines = models.trim().split('\n').slice(1); // Pular cabe√ßalho
        modelLines.forEach(line => {
          if (line.trim()) {
            const parts = line.trim().split(/\s+/);
            const modelName = parts[0];
            const isActive = this.currentProject?.model === modelName;
            const status = isActive ? 'üü¢ ATIVO' : '‚ö™ Dispon√≠vel';
            Logger.info(`   ${status} ${modelName}`);
          }
        });
        
      } catch (error) {
        Logger.warn('‚ö†Ô∏è N√£o foi poss√≠vel verificar status do Ollama');
      }
      
      Logger.newline();
      Logger.info('üéØ Comandos dispon√≠veis:');
      Logger.info('   ‚Ä¢ llm chat - Iniciar conversa com IA');
      Logger.info('   ‚Ä¢ llm change-model <nome> - Trocar modelo');
      Logger.info('   ‚Ä¢ llm status - Ver este status novamente');
      
    } catch (error) {
      Logger.error('Erro ao mostrar status:', error);
      throw error;
    }
  }

  /**
   * Define o modelo base padr√£o para todos os projetos
   */
  async setDefaultModel(modelName?: string): Promise<void> {
    if (!modelName) {
      // Usar sele√ß√£o interativa
      Logger.info('‚öôÔ∏è Vamos escolher um modelo padr√£o para todos os projetos!');
      modelName = await this.modelManager.selectModel();
    }

    Logger.info(`‚öôÔ∏è Definindo modelo padr√£o: ${modelName}`);
    
    // Verificar disponibilidade do modelo e baixar se necess√°rio
    modelName = await this.modelManager.ensureModelReady(modelName);

    await this.configManager.setDefaultModel(modelName);
    Logger.success(`‚úÖ Modelo padr√£o definido: ${modelName}`);
  }

  /**
   * Lista modelos LLMs dispon√≠veis
   */
  async listModels(): Promise<void> {
    Logger.info('üìã Modelos dispon√≠veis:');
    
    const availableModels = await this.ollamaManager.listModels();
    const defaultModel = await this.configManager.getDefaultModel();
    
    availableModels.forEach((model, index) => {
      const isDefault = model.name === defaultModel;
      const status = isDefault ? ' (padr√£o)' : '';
      Logger.info(`  ${index + 1}. ${model.name}${status}`);
      Logger.info(`     Descri√ß√£o: ${model.description}`);
      Logger.info(`     Tamanho: ${model.size}`);
      Logger.info(`     Compatibilidade: ${model.compatibility}`);
      Logger.info('');
    });
  }

  /**
   * Inicia modo conversacional
   */
  async startChat(specificModel?: string): Promise<void> {
    Logger.info('üí¨ Iniciando modo conversacional...');
    
    // Verificar se o projeto est√° inicializado
    if (!this.currentProject) {
      const projectPath = process.cwd();
      const isInitialized = await this.projectManager.isProjectInitialized(projectPath);
      
      if (!isInitialized) {
        Logger.warn('‚ö†Ô∏è Projeto n√£o inicializado!');
        Logger.info('üìÅ Para usar o chat, voc√™ precisa inicializar o projeto primeiro.');
        Logger.info('üí° Execute o comando: llm init');
        Logger.newline();
        
        // Perguntar se quer inicializar
        const { shouldInit } = await this.conversationManager.askUser({
          type: 'confirm',
          message: 'Deseja inicializar o projeto agora?',
          default: true
        });
        
        if (shouldInit) {
          Logger.info('üöÄ Inicializando projeto...');
          await this.initializeProject({});
          Logger.success('‚úÖ Projeto inicializado! Iniciando chat...');
        } else {
          Logger.info('‚ùå Chat cancelado. Execute "llm init" quando estiver pronto.');
          return;
        }
      } else {
        // Carregar projeto existente
        this.currentProject = await this.projectManager.loadProject(projectPath);
        Logger.info(`üìÅ Projeto carregado: ${this.currentProject.name}`);
      }
    }
    
    // Determinar modelo a ser usado
    let modelToUse = specificModel;
    
    if (!modelToUse && this.currentProject) {
      modelToUse = this.currentProject.model;
    }
    
    if (!modelToUse) {
      const defaultModel = await this.configManager.getDefaultModel();
      if (defaultModel) {
        modelToUse = defaultModel;
      } else {
        // Nenhum modelo configurado, usar sele√ß√£o interativa
        Logger.info('ü§ñ Nenhum modelo configurado. Vamos escolher um!');
        modelToUse = await this.modelManager.selectModel();
      }
    }

    // Verificar se o modelo est√° dispon√≠vel
    const availableModels = await this.ollamaManager.listModels();
    const modelExists = availableModels.some(m => m.name === modelToUse);

    if (!modelExists) {
      Logger.warn(`‚ö†Ô∏è Modelo "${modelToUse}" n√£o encontrado. Tentando baixar...`);
      try {
        await this.ollamaManager.downloadModelWithProgress(modelToUse);
        Logger.success(`‚úÖ Modelo ${modelToUse} baixado com sucesso!`);
      } catch (downloadError) {
        throw new Error(`Modelo "${modelToUse}" n√£o encontrado e n√£o p√¥de ser baixado. Use "ollama pull ${modelToUse}" manualmente.`);
      }
    }

    // Configurar modelo no projeto se n√£o estiver configurado
    if (this.currentProject && this.currentProject.model !== modelToUse) {
      await this.modelManager.setProjectModel(this.currentProject.path, modelToUse);
      this.currentProject.model = modelToUse;
      await this.projectManager.updateProjectModel(this.currentProject.path, modelToUse);
    }

    // Inicializar modelo (sem duplicar inicializa√ß√£o)
    await this.ollamaManager.initialize(modelToUse);
    
    Logger.success(`ü§ñ Conectado ao modelo: ${modelToUse}`);
    Logger.info('üí° Digite suas perguntas ou comandos. Use "/help" para ver comandos dispon√≠veis.');
    
    // Iniciar conversa
    await this.conversationManager.startConversation(modelToUse);
  }

  /**
   * Cria uma nova funcionalidade no projeto
   */
  async createFeature(type: string, name: string, description?: string): Promise<void> {
    if (!this.currentProject) {
      throw new Error('Nenhum projeto ativo. Execute "llm init" primeiro.');
    }

    Logger.info(`üî® Criando funcionalidade: ${type} - ${name}`);
    
    // Gerar funcionalidade via LLM
    const prompt = `Crie uma funcionalidade do tipo "${type}" chamada "${name}"${description ? ` com descri√ß√£o: "${description}"` : ''} para um projeto ${this.currentProject.language}/${this.currentProject.framework}.`;
    
    const response = await this.modelManager.sendPrompt(this.currentProject.model!, prompt);
    
    // Aplicar mudan√ßas
    await this.fileManager.applyChanges(response.changes);
    
    Logger.success(`‚úÖ Funcionalidade "${name}" criada com sucesso!`);
  }

  /**
   * Edita um arquivo existente
   */
  async editFile(filePath: string, instruction: string): Promise<void> {
    if (!this.currentProject) {
      throw new Error('Nenhum projeto ativo. Execute "llm init" primeiro.');
    }

    Logger.info(`‚úèÔ∏è Editando arquivo: ${filePath}`);
    
    // Ler arquivo atual
    const currentContent = await this.fileManager.readFile(filePath);
    
    // Gerar edi√ß√£o via LLM
    const prompt = `Edite o arquivo "${filePath}" com a seguinte instru√ß√£o: "${instruction}". Arquivo atual:\n\`\`\`\n${currentContent}\n\`\`\``;
    
    const response = await this.modelManager.sendPrompt(this.currentProject.model!, prompt);
    
    // Aplicar mudan√ßas
    await this.fileManager.applyChanges(response.changes);
    
    Logger.success(`‚úÖ Arquivo "${filePath}" editado com sucesso!`);
  }

  /**
   * Desfaz altera√ß√µes recentes
   */
  async rollback(numberOfChanges: number = 1): Promise<void> {
    if (!this.currentProject) {
      throw new Error('Nenhum projeto ativo. Execute "llm init" primeiro.');
    }

    Logger.info(`‚Ü©Ô∏è Desfazendo ${numberOfChanges} altera√ß√£o(√µes)...`);
    
    const undone = await this.fileManager.rollback(numberOfChanges);
    Logger.success(`‚úÖ ${undone} altera√ß√£o(√µes) desfeita(s) com sucesso!`);
  }
}
