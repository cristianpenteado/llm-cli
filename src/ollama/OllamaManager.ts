import { exec } from 'child_process';
import { promisify } from 'util';
import { Logger } from '../utils/Logger';
import { ModelConfig } from '../types';

const execAsync = promisify(exec);

export class OllamaManager {
  private cache = new Map<string, { response: string; timestamp: number; ttl: number }>();
  private modelCache = new Map<string, ModelConfig[]>();
  private lastModelList = 0;
  private readonly CACHE_TTL = 300000; // 5 minutos
  private readonly MODEL_LIST_CACHE_TTL = 60000; // 1 minuto
  private defaultModel = 'phi3:mini';
  private isInitialized = false;
  private isFirstRun = true; // Adicionado para controlar a primeira execu√ß√£o
  private activeSession: any = null; // Sess√£o ativa com o modelo
  private backgroundProcess: any = null; // Processo em background
  private persistentModelProcess: any = null; // Processo persistente do modelo
  private verboseLogs: boolean;

  constructor(verboseLogs: boolean = false) {
    this.verboseLogs = verboseLogs;
    // Inicializar cache limpo
  }

  /**
   * Inicializa o gerenciador Ollama
   */
  async initialize(): Promise<void> {
    if (this.isInitialized) {
      return;
    }

    try {
      Logger.ollama('üöÄ Inicializando gerenciador Ollama...');

      // Verificar instala√ß√£o
      await this.checkOllamaInstallation();

      // Verificar servidor
      await this.checkOllamaServer();

      // Garantir modelo padr√£o
      await this.ensureDefaultModel();

      // Iniciar modelo em background automaticamente (n√£o-bloqueante)
      this.startModelInBackgroundAsync(this.defaultModel);

      this.isInitialized = true;
      Logger.success('‚úÖ Gerenciador Ollama inicializado com modelo padr√£o');

    } catch (error) {
      Logger.error('Erro ao inicializar gerenciador Ollama:', error);
      throw error;
    }
  }

  /**
   * Verifica se o Ollama est√° instalado
   */
  private async checkOllamaInstallation(): Promise<void> {
    try {
      const { stdout } = await execAsync('ollama --version');
      Logger.ollama(`‚úÖ Ollama instalado: ${stdout.trim()}`);
    } catch (error) {
      Logger.error('‚ùå Ollama n√£o encontrado!');
      Logger.info('üìã Para instalar o Ollama, visite: https://ollama.ai');
      Logger.info('üíª Ou execute: curl -fsSL https://ollama.ai/install.sh | sh');
      throw new Error('Ollama n√£o est√° instalado. √â um pr√©-requisito para este projeto.');
    }
  }

  /**
   * Verifica se o servidor Ollama est√° rodando
   */
  private async checkOllamaServer(): Promise<void> {
    try {
      // Tentar listar modelos para verificar se o servidor est√° respondendo
      const { stdout } = await execAsync('ollama list');
      Logger.ollama('‚úÖ Servidor Ollama est√° rodando');
    } catch (error) {
      Logger.warn('‚ö†Ô∏è Servidor Ollama n√£o est√° respondendo, verificando status...');
      
      try {
        // Tentar iniciar servidor Ollama
        const { stdout, stderr } = await execAsync('ollama serve', { timeout: 5000 });
        Logger.success('‚úÖ Servidor Ollama iniciado');
        
      } catch (startError: any) {
        // Verificar se o erro √© de porta em uso (Ollama j√° est√° rodando)
        if (startError.stderr && startError.stderr.includes('address already in use')) {
          Logger.ollama('‚úÖ Porta 11434 em uso - Ollama j√° est√° rodando em background');
          // N√£o aguardar, continuar direto - o servidor pode demorar para responder
          return;
        }
        
        // Se n√£o for erro de porta em uso, tentar verificar se o servidor j√° est√° rodando
        try {
          await this.waitForServerReady();
          Logger.ollama('‚úÖ Servidor Ollama j√° estava rodando');
          return;
        } catch (waitError) {
          Logger.error('‚ùå Erro ao iniciar servidor Ollama:', startError);
          throw new Error('Falha ao iniciar servidor Ollama. Execute manualmente: ollama serve');
        }
      }
    }
  }

  /**
   * Aguarda o servidor estar pronto
   */
  private async waitForServerReady(): Promise<void> {
    let attempts = 0;
    const maxAttempts = 30;
    
    while (attempts < maxAttempts) {
      try {
        const { stdout } = await execAsync('ollama list');
        if (stdout && stdout.trim()) {
          return;
        }
      } catch (error) {
        // Servidor ainda n√£o est√° pronto
      }
      
      attempts++;
      await new Promise(resolve => setTimeout(resolve, 1000));
    }
    
    throw new Error('Timeout aguardando servidor Ollama ficar pronto');
  }

  /**
   * Garante que o modelo padr√£o est√° dispon√≠vel
   */
  private async ensureDefaultModel(): Promise<void> {
    try {
      Logger.ollama(`üîç Verificando se modelo padr√£o ${this.defaultModel} est√° dispon√≠vel...`);
      
      const models = await this.listModels();
      Logger.ollama(`üìã Modelos encontrados: ${models.length}`);
      
      if (models.length > 0) {
        Logger.ollama(`üìù Modelos: ${models.map(m => m.name).join(', ')}`);
      }
      
      const defaultModel = models.find(m => m.name === this.defaultModel);
      
      if (defaultModel) {
        Logger.ollama(`‚úÖ Modelo padr√£o ${this.defaultModel} j√° est√° dispon√≠vel`);
        return;
      }
      
      Logger.ollama(`‚ùå Modelo padr√£o ${this.defaultModel} n√£o encontrado. Mostrando seletor...`);
      
      // Perguntar ao usu√°rio se quer baixar o modelo padr√£o
      const shouldDownload = await this.askUserToDownloadDefault();
      
      if (shouldDownload) {
        Logger.ollama(`üì• Baixando modelo padr√£o ${this.defaultModel}...`);
        await this.downloadModelWithProgress(this.defaultModel);
        Logger.success(`‚úÖ Modelo padr√£o ${this.defaultModel} baixado com sucesso`);
      } else {
        Logger.ollama(`ü§ñ Usu√°rio escolheu selecionar outro modelo...`);
        // Permitir que o usu√°rio escolha outro modelo
        const selectedModel = await this.selectModelInteractively();
        if (selectedModel) {
          Logger.ollama(`‚úÖ Usando modelo selecionado: ${selectedModel}`);
          // Atualizar modelo padr√£o
          this.defaultModel = selectedModel;
        } else {
          throw new Error('Nenhum modelo selecionado. √â necess√°rio ter pelo menos um modelo dispon√≠vel.');
        }
      }
      
    } catch (error) {
      Logger.error(`Erro ao garantir modelo padr√£o ${this.defaultModel}:`, error);
      throw error;
    }
  }

  /**
   * Pergunta ao usu√°rio se quer baixar o modelo padr√£o
   */
  private async askUserToDownloadDefault(): Promise<boolean> {
    try {
      const inquirer = (await import('inquirer')).default;
      
      const { action } = await inquirer.prompt([
        {
          type: 'list',
          name: 'action',
          message: `Modelo padr√£o ${this.defaultModel} n√£o encontrado. O que voc√™ gostaria de fazer?`,
          choices: [
            { name: `üì• Baixar ${this.defaultModel} (recomendado)`, value: 'download' },
            { name: 'ü§ñ Escolher outro modelo dispon√≠vel', value: 'select' },
            { name: '‚ùå Cancelar', value: 'cancel' }
          ],
          default: 'download'
        }
      ]);

      if (action === 'cancel') {
        throw new Error('Opera√ß√£o cancelada pelo usu√°rio');
      }

      return action === 'download';
    } catch (error) {
      // Se inquirer falhar, usar fallback
      Logger.info(`Modelo padr√£o ${this.defaultModel} n√£o encontrado.`);
      Logger.info('Para baixar manualmente, execute: ollama pull phi3:mini');
      return false;
    }
  }

  /**
   * Permite ao usu√°rio escolher um modelo interativamente
   */
  private async selectModelInteractively(): Promise<string | null> {
    try {
      const inquirer = (await import('inquirer')).default;
      
      // Listar modelos dispon√≠veis
      const availableModels = await this.listAvailableModels();
      
      if (availableModels.length === 0) {
        Logger.warn('‚ö†Ô∏è Nenhum modelo dispon√≠vel. Baixando modelo padr√£o...');
        await this.downloadModelWithProgress(this.defaultModel);
        return this.defaultModel;
      }

      const { selectedModel } = await inquirer.prompt([
        {
          type: 'list',
          name: 'selectedModel',
          message: 'Escolha um modelo para usar:',
          choices: [
            ...availableModels.map(model => ({
              name: `${model} (recomendado)`,
              value: model
            })),
            { name: 'üì• Baixar novo modelo', value: 'download_new' }
          ]
        }
      ]);

      if (selectedModel === 'download_new') {
        const { newModelName } = await inquirer.prompt([
          {
            type: 'input',
            name: 'newModelName',
            message: 'Digite o nome do modelo para baixar (ex: llama3.2:3b):',
            default: 'llama3.2:3b'
          }
        ]);

        if (newModelName.trim()) {
          Logger.ollama(`üì• Baixando modelo ${newModelName}...`);
          await this.downloadModelWithProgress(newModelName.trim());
          return newModelName.trim();
        }
      }

      return selectedModel;
    } catch (error) {
      Logger.error('Erro ao selecionar modelo:', error);
      return null;
    }
  }

  /**
   * Lista modelos dispon√≠veis para download
   */
  private async listAvailableModels(): Promise<string[]> {
    try {
      // Modelos populares e leves
      return [
        'phi3:mini',
        'llama3.2:3b',
        'mistral:7b',
        'gemma2:2b',
        'qwen2.5:0.5b',
        'codellama:7b'
      ];
    } catch (error) {
      return ['phi3:mini']; // Fallback
    }
  }

  /**
   * Faz download de um modelo com progresso visual
   */
  async downloadModelWithProgress(modelName: string): Promise<void> {
    try {
      Logger.ollama(`üì• Iniciando download do modelo ${modelName}...`);
      
      // Usar spawn para capturar output em tempo real
      const { spawn } = await import('child_process');
      
      return new Promise((resolve, reject) => {
        const ollamaProcess = spawn('ollama', ['pull', modelName]);
        
        let output = '';
        let lastProgress = '';
        
        ollamaProcess.stdout.on('data', (data) => {
          const text = data.toString();
          output += text;
          
          // Mostrar progresso em tempo real
          if (text.includes('pulling')) {
            const progressMatch = text.match(/(\d+\.?\d*)%/);
            if (progressMatch && progressMatch[1] !== lastProgress) {
              lastProgress = progressMatch[1];
              process.stdout.write(`\rüì• Download: ${lastProgress}%`);
            }
          }
        });
        
        ollamaProcess.stderr.on('data', (data) => {
          const text = data.toString();
          if (text.includes('pulling')) {
            // Mostrar progresso do stderr tamb√©m
            const progressMatch = text.match(/(\d+\.?\d*)%/);
            if (progressMatch && progressMatch[1] !== lastProgress) {
              lastProgress = progressMatch[1];
              process.stdout.write(`\rüì• Download: ${lastProgress}%`);
            }
          }
        });
        
        ollamaProcess.on('close', (code) => {
          process.stdout.write('\n'); // Nova linha ap√≥s progresso
          
          if (code === 0) {
            Logger.ollama(`‚úÖ Download do modelo ${modelName} conclu√≠do`);
            // Limpar cache de modelos para refletir a mudan√ßa
            this.modelCache.clear();
            resolve();
          } else {
            reject(new Error(`Download falhou com c√≥digo ${code}`));
          }
        });
        
        ollamaProcess.on('error', (error) => {
          reject(error);
        });
        
        // Timeout de 10 minutos
        setTimeout(() => {
          ollamaProcess.kill();
          reject(new Error('Timeout: download demorou mais de 10 minutos'));
        }, 600000);
      });
      
    } catch (error) {
      Logger.error(`Erro ao fazer download do modelo ${modelName}:`, error);
      throw error;
    }
  }

  /**
   * Gera resposta do modelo com sess√£o cont√≠nua
   */
  async generateResponse(modelName: string, prompt: string, context?: string): Promise<{ response: string }> {
    // Garantir inicializa√ß√£o
    if (!this.isInitialized) {
      await this.initialize();
    }

    try {
      // Verificar se o modelo existe, se n√£o, tentar baixar
      await this.ensureModelAvailable(modelName);
      
      // Iniciar ou usar sess√£o ativa
      if (!this.activeSession || this.activeSession.modelName !== modelName) {
        await this.startModelSession(modelName);
      }

      // Enviar prompt para a sess√£o ativa
      const response = await this.sendToActiveSession(prompt, context);

      // Cache da resposta
      const cacheKey = `${modelName}:${this.hashPrompt(prompt)}`;
      this.cache.set(cacheKey, {
        response: response,
        timestamp: Date.now(),
        ttl: this.CACHE_TTL
      });

      // Marcar que n√£o √© mais a primeira execu√ß√£o
      this.isFirstRun = false;

      return { response: response };
    } catch (error) {
      Logger.error('Erro ao gerar resposta:', error);
      throw error;
    }
  }

  /**
   * Garante que o modelo est√° dispon√≠vel
   */
  private async ensureModelAvailable(modelName: string): Promise<void> {
    try {
      // Tentar listar modelos com retry
      let models: ModelConfig[] = [];
      let attempts = 0;
      const maxAttempts = 3;
      
      while (attempts < maxAttempts) {
        try {
          models = await this.listModels();
          break;
        } catch (error) {
          attempts++;
          if (attempts >= maxAttempts) {
            throw new Error(`N√£o foi poss√≠vel conectar ao servidor Ollama ap√≥s ${maxAttempts} tentativas`);
          }
          // Aguardar um pouco antes de tentar novamente
          await new Promise(resolve => setTimeout(resolve, 2000));
        }
      }
      
      const model = models.find(m => m.name === modelName);
      
      if (!model) {
        if (modelName === this.defaultModel) {
          // Para o modelo padr√£o, sempre tentar baixar
          await this.downloadModelWithProgress(modelName);
        } else {
          // Para outros modelos, informar que precisa baixar
          throw new Error(`Modelo ${modelName} n√£o encontrado. Use "ollama pull ${modelName}" para baix√°-lo.`);
        }
      }
    } catch (error) {
      Logger.error(`Erro ao verificar modelo ${modelName}:`, error);
      throw error;
    }
  }

  /**
   * Chama Ollama com otimiza√ß√µes
   */
  private async callOllama(modelName: string, prompt: string, context?: string): Promise<string> {
    // Para conversas simples, usar apenas o prompt
    let fullPrompt = prompt;
    
    // Adicionar contexto apenas se for necess√°rio e n√£o for conversa simples
    if (context && context.trim() && !this.isSimplePrompt(prompt)) {
      fullPrompt = `${context}\n\nPergunta: ${prompt}`;
    }
    
    // Usar spawn para melhor controle do processo
    const { spawn } = await import('child_process');
    
    return new Promise((resolve, reject) => {
      const ollamaProcess = spawn('ollama', ['run', modelName, fullPrompt]);
      
      let stdout = '';
      let stderr = '';
      
      ollamaProcess.stdout.on('data', (data) => {
        stdout += data.toString();
      });
      
      ollamaProcess.stderr.on('data', (data) => {
        stderr += data.toString();
      });
      
      ollamaProcess.on('close', (code) => {
        if (code === 0) {
          resolve(stdout.trim());
        } else {
          reject(new Error(`Ollama process exited with code ${code}. Stderr: ${stderr}`));
        }
      });
      
      ollamaProcess.on('error', (error) => {
        reject(error);
      });
      
      // Timeout fixo de 60 segundos para primeira execu√ß√£o
      const timeoutId = setTimeout(() => {
        ollamaProcess.kill('SIGTERM');
        reject(new Error('Timeout: resposta demorou mais de 60s'));
      }, 60000);
      
      // Limpar timeout se o processo terminar antes
      ollamaProcess.on('close', () => clearTimeout(timeoutId));
      ollamaProcess.on('error', () => clearTimeout(timeoutId));
    });
  }

  /**
   * Verifica se √© um prompt simples (conversa)
   */
  private isSimplePrompt(prompt: string): boolean {
    const lowerPrompt = prompt.toLowerCase();
    
    // Palavras-chave que indicam conversa simples
    const simpleKeywords = [
      'ol√°', 'oi', 'bom dia', 'boa tarde', 'boa noite',
      'como voc√™ est√°', 'tudo bem', 'beleza', 'valeu',
      'obrigado', 'obrigada', 'obg', 'thanks', 'thank you',
      'o que √©', 'como funciona', 'explique', 'descreva',
      'ajuda', 'ajude', 'pode me ajudar'
    ];
    
    return simpleKeywords.some(keyword => lowerPrompt.includes(keyword));
  }

  /**
   * Lista modelos com cache otimizado
   */
  async listModels(): Promise<ModelConfig[]> {
    const now = Date.now();
    
    // Verificar cache de modelos
    if (this.modelCache.has('all') && (now - this.lastModelList) < this.MODEL_LIST_CACHE_TTL) {
      return this.modelCache.get('all') || [];
    }

    try {
      // Usar comando sem --json para compatibilidade com vers√µes antigas
      const { stdout } = await execAsync('ollama list', {
        timeout: 5000, // 5s timeout para listar modelos
        maxBuffer: 1024 * 1024
      });

      // Parsear sa√≠da do comando ollama list
      const models: ModelConfig[] = this.parseOllamaListOutput(stdout);
      
      // Cache dos modelos
      this.modelCache.set('all', models);
      this.lastModelList = now;
      
      return models;
      
    } catch (error) {
      Logger.error('Erro ao listar modelos:', error);
      return [];
    }
  }

  /**
   * Parseia a sa√≠da do comando ollama list
   */
  private parseOllamaListOutput(output: string): ModelConfig[] {
    try {
      const lines = output.trim().split('\n');
      const models: ModelConfig[] = [];
      
      // Pular cabe√ßalho se existir
      const startIndex = lines[0].includes('NAME') ? 1 : 0;
      
      for (let i = startIndex; i < lines.length; i++) {
        const line = lines[i].trim();
        if (!line) continue;
        
        // Parsear linha do formato: NAME                TAG                 SIZE       ID
        // Exemplo: phi3:mini         latest            3.8 GB     a1b2c3d4
        const parts = line.split(/\s+/);
        if (parts.length >= 3) {
          const name = parts[0];
          const tag = parts[1];
          const size = parts.slice(2, -1).join(' '); // Tamanho pode ter espa√ßos
          const id = parts[parts.length - 1];
          
          models.push({
            name: name,
            description: `Modelo Ollama: ${name}:${tag}`,
            size: size,
            compatibility: 'Ollama',
            parameters: 0, // N√£o dispon√≠vel na vers√£o antiga
            contextLength: 0, // N√£o dispon√≠vel na vers√£o antiga
            isLocal: true,
            status: 'ready'
          });
        }
      }
      
      return models;
    } catch (error) {
      Logger.warn('Erro ao parsear sa√≠da do ollama list, retornando lista vazia');
      return [];
    }
  }



  /**
   * Inicia o modelo em modo persistente para respostas imediatas
   */
  private startModelInBackgroundAsync(modelName: string): void {
    // Executar em background sem bloquear
    setImmediate(async () => {
      try {
        // Parar processo anterior se existir
        if (this.persistentModelProcess) {
          if (this.verboseLogs) {
            Logger.ollama(`üîÑ [LOGS] Parando processo anterior PID: ${this.persistentModelProcess.pid}`);
          }
          this.persistentModelProcess.kill('SIGTERM');
          this.persistentModelProcess = null;
        }

        if (this.verboseLogs) {
          Logger.ollama(`üöÄ [LOGS] Iniciando spawn do modelo ${modelName}`);
        }

        // Iniciar modelo em modo persistente com stdio para comunica√ß√£o
        const { spawn } = await import('child_process');
        
        this.persistentModelProcess = spawn('ollama', ['run', modelName], {
          stdio: ['pipe', 'pipe', 'pipe'], // Manter stdin/stdout para comunica√ß√£o
          detached: false // Processo controlado
        });

        if (this.verboseLogs) {
          Logger.ollama(`üîç [LOGS] Processo spawnado com PID: ${this.persistentModelProcess.pid}`);
          Logger.ollama(`üîç [LOGS] stdio configurado: pipe, pipe, pipe`);
        }

        // Configurar listeners para o processo
        this.persistentModelProcess.on('error', (error: Error) => {
          if (this.verboseLogs) {
            Logger.ollama(`‚ùå [LOGS] Erro no processo: ${error}`);
          }
          Logger.error('Erro no processo persistente:', error);
        });

        this.persistentModelProcess.on('exit', (code: number | null, signal: NodeJS.Signals | null) => {
          if (this.verboseLogs) {
            Logger.ollama(`‚ö†Ô∏è [LOGS] Processo encerrado: c√≥digo ${code}, sinal ${signal}`);
          }
          Logger.warn(`Processo persistente encerrado: c√≥digo ${code}, sinal ${signal}`);
          this.persistentModelProcess = null;
        });

        Logger.ollama(`‚úÖ Modelo ${modelName} iniciado em modo persistente`);
        
      } catch (error) {
        if (this.verboseLogs) {
          Logger.ollama(`‚ùå [LOGS] Erro ao iniciar processo: ${error}`);
        }
        Logger.warn(`Modelo ${modelName} n√£o p√¥de ser iniciado em modo persistente:`, error);
      }
    });
  }

  /**
   * Envia prompt para o processo persistente para resposta imediata
   */
  private async sendToPersistentProcess(prompt: string): Promise<string> {
    if (this.verboseLogs) {
      Logger.ollama(`üîç [LOGS] Enviando prompt para processo persistente: "${prompt}"`);
      Logger.ollama(`üîç [LOGS] Processo PID: ${this.persistentModelProcess?.pid}`);
      Logger.ollama(`üîç [LOGS] Processo alive: ${this.persistentModelProcess?.killed === false}`);
    }

    return new Promise((resolve, reject) => {
      if (!this.persistentModelProcess || !this.persistentModelProcess.pid) {
        const error = 'Processo persistente n√£o est√° dispon√≠vel';
        if (this.verboseLogs) Logger.ollama(`‚ùå [LOGS] ${error}`);
        reject(new Error(error));
        return;
      }

      let response = '';
      let hasResponse = false;
      let stderrData = '';
      
      if (this.verboseLogs) {
        Logger.ollama(`üîç [LOGS] Configurando listeners para stdout, stderr e stdin`);
      }
      
      // Capturar stderr para debug
      this.persistentModelProcess.stderr.on('data', (data: Buffer) => {
        const dataStr = data.toString();
        stderrData += dataStr;
        if (this.verboseLogs) {
          Logger.ollama(`‚ö†Ô∏è [LOGS] Recebido stderr: "${dataStr}"`);
        }
      });
      
      // Capturar stdout para resposta
      this.persistentModelProcess.stdout.on('data', (data: Buffer) => {
        const dataStr = data.toString();
        response += dataStr;
        if (this.verboseLogs) {
          Logger.ollama(`üì• [LOGS] Recebido stdout: "${dataStr}"`);
          Logger.ollama(`üì• [LOGS] Resposta acumulada: "${response}"`);
        }
        
        // Verificar se temos uma resposta v√°lida
        if (response.trim() && !hasResponse) {
          hasResponse = true;
          if (this.verboseLogs) {
            Logger.ollama(`‚úÖ [LOGS] Resposta completa recebida: "${response.trim()}"`);
          }
          resolve(response.trim());
        }
      });
      
      // Timeout de 15 segundos para resposta completa
      const timeoutId = setTimeout(() => {
        if (!hasResponse) {
          const error = `Timeout: resposta demorou mais de 15s. stderr: "${stderrData}"`;
          if (this.verboseLogs) Logger.ollama(`‚è∞ [LOGS] ${error}`);
          reject(new Error(error));
        }
      }, 15000);
      
      // Enviar prompt
      if (this.verboseLogs) {
        Logger.ollama(`üì§ [LOGS] Enviando prompt via stdin: "${prompt}"`);
      }
      
      // Adicionar quebra de linha e aguardar um pouco
      this.persistentModelProcess.stdin.write(prompt + '\n');
      
      // Aguardar um pouco mais para o modelo processar
      setTimeout(() => {
        if (!hasResponse) {
          if (this.verboseLogs) {
            Logger.ollama(`‚ö†Ô∏è [LOGS] Aguardando resposta... (5s)`);
          }
        }
      }, 5000);
      
      // Se n√£o houver resposta em 10s, tentar detectar se o modelo est√° processando
      setTimeout(() => {
        if (!hasResponse) {
          if (this.verboseLogs) {
            Logger.ollama(`‚ö†Ô∏è [LOGS] Aguardando resposta... (10s)`);
            Logger.ollama(`‚ö†Ô∏è [LOGS] stderr acumulado: "${stderrData}"`);
          }
        }
      }, 10000);
    });
  }

  /**
   * Envia prompt para o processo em background
   */
  private async sendToBackgroundProcess(prompt: string): Promise<string> {
    return new Promise((resolve, reject) => {
      if (!this.backgroundProcess || !this.backgroundProcess.pid) {
        reject(new Error('Processo em background n√£o est√° dispon√≠vel'));
        return;
      }

      // Usar exec para enviar prompt ao modelo em background
      const { exec } = require('child_process');
      const { promisify } = require('util');
      const execAsync = promisify(exec);
      
      const command = `ollama run ${this.activeSession.modelName} "${prompt.replace(/"/g, '\\"')}"`;
      
      execAsync(command, {
        timeout: 60000,
        maxBuffer: 1024 * 1024
      }).then(({ stdout }: any) => {
        resolve(stdout.trim());
      }).catch((error: any) => {
        reject(error);
      });
    });
  }

  /**
   * Inicia uma sess√£o cont√≠nua com o modelo
   */
  async startModelSession(modelName: string): Promise<void> {
    try {
      // Parar sess√£o anterior se existir
      if (this.activeSession) {
        await this.stopModelSession();
      }

      // Para vers√µes antigas do Ollama, usar exec em vez de spawn
      // pois ollama run n√£o funciona bem com stdin/stdout
      this.activeSession = {
        modelName,
        process: null,
        isReady: true
      };
      
    } catch (error) {
      Logger.error(`Erro ao iniciar sess√£o com ${modelName}:`, error);
      throw error;
    }
  }

  /**
   * Para a sess√£o ativa do modelo
   */
  async stopModelSession(): Promise<void> {
    if (this.activeSession) {
      try {
        this.activeSession.process.kill('SIGTERM');
        this.activeSession = null;
      } catch (error) {
        Logger.warn('Erro ao parar sess√£o do modelo:', error);
      }
    }
  }

  /**
   * Aguarda a sess√£o estar pronta
   */
  private async waitForSessionReady(): Promise<void> {
    if (!this.activeSession) return;
    
    return new Promise((resolve) => {
      const checkReady = () => {
        if (this.activeSession && this.activeSession.process.pid) {
          this.activeSession.isReady = true;
          resolve();
        } else {
          setTimeout(checkReady, 100);
        }
      };
      checkReady();
    });
  }

  /**
   * Envia prompt para a sess√£o ativa
   */
  private async sendToActiveSession(prompt: string, context?: string): Promise<string> {
    if (!this.activeSession || !this.activeSession.isReady) {
      throw new Error('Sess√£o do modelo n√£o est√° ativa');
    }

    try {
      // Para conversas simples, usar apenas o prompt
      let fullPrompt = prompt;
      
      // Adicionar contexto apenas se for necess√°rio e n√£o for conversa simples
      if (context && context.trim() && !this.isSimplePrompt(prompt)) {
        fullPrompt = `${context}\n\nPergunta: ${prompt}`;
      }
      
      // Tentar usar o processo persistente primeiro para resposta imediata
      if (this.persistentModelProcess && this.persistentModelProcess.pid) {
        try {
          if (this.verboseLogs) {
            Logger.ollama(`üîç [LOGS] Tentando processo persistente primeiro...`);
          }
          
          // Timeout reduzido para 8 segundos
          const response = await Promise.race([
            this.sendToPersistentProcess(fullPrompt),
            new Promise<string>((_, reject) => 
              setTimeout(() => reject(new Error('Processo persistente lento')), 8000)
            )
          ]);
          
          if (this.verboseLogs) {
            Logger.ollama(`‚úÖ [LOGS] Resposta do processo persistente: "${response}"`);
          }
          
          return response;
        } catch (persistentError) {
          if (this.verboseLogs) {
            Logger.ollama(`‚ö†Ô∏è [LOGS] Processo persistente falhou: ${persistentError}`);
          }
          Logger.warn('Processo persistente falhou, usando fallback...');
        }
      }
      
      // Fallback: usar exec diretamente
      const { exec } = await import('child_process');
      const { promisify } = await import('util');
      const execAsync = promisify(exec);
      
      const command = `ollama run ${this.activeSession.modelName} "${fullPrompt.replace(/"/g, '\\"')}"`;
      
      // Timeout progressivo: 120s para primeira execu√ß√£o, 60s para subsequentes
      const timeout = this.isFirstRun ? 120000 : 60000;
      
      const { stdout } = await execAsync(command, {
        timeout: timeout,
        maxBuffer: 1024 * 1024 // 1MB buffer
      });

      // Marcar que n√£o √© mais a primeira execu√ß√£o
      this.isFirstRun = false;

      return stdout.trim();
    } catch (error) {
      Logger.error(`Erro ao enviar prompt para ${this.activeSession.modelName}:`, error);
      throw error;
    }
  }



  /**
   * Hash simples para cache de prompts
   */
  private hashPrompt(prompt: string): string {
    let hash = 0;
    for (let i = 0; i < prompt.length; i++) {
      const char = prompt.charCodeAt(i);
      hash = ((hash << 5) - hash) + char;
      hash = hash & hash; // Convert to 32bit integer
    }
    return hash.toString();
  }

  /**
   * Promise com timeout
   */
  private timeoutPromise<T>(ms: number, message: string): Promise<T> {
    return new Promise((_, reject) => {
      setTimeout(() => reject(new Error(message)), ms);
    });
  }

  /**
   * Formata tamanho do modelo para exibi√ß√£o
   */
  private formatModelSize(bytes: number): string {
    const sizes = ['B', 'KB', 'MB', 'GB', 'TB'];
    if (bytes === 0) return '0 B';
    
    const i = Math.floor(Math.log(bytes) / Math.log(1024));
    return Math.round(bytes / Math.pow(1024, i) * 100) / 100 + ' ' + sizes[i];
  }

  /**
   * Limpa cache manualmente
   */
  clearCache(): void {
    this.cache.clear();
    this.modelCache.clear();
    Logger.ollama('üóëÔ∏è Cache limpo');
  }

  /**
   * Obt√©m o modelo padr√£o
   */
  getDefaultModel(): string {
    return this.defaultModel;
  }
}
