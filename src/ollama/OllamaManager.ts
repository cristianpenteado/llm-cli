import { exec } from 'child_process';
import { promisify } from 'util';
import { Logger } from '../utils/Logger';
import { ModelConfig } from '../types';

const execAsync = promisify(exec);

export class OllamaManager {
  private cache = new Map<string, { response: string; timestamp: number; ttl: number }>();
  private modelCache = new Map<string, ModelConfig[]>();
  private lastModelList = 0;
  private readonly CACHE_TTL = 30000; // 30 segundos
  private readonly MODEL_LIST_CACHE_TTL = 10000; // 10 segundos
  private defaultModel = 'phi3:mini';
  private isInitialized = false;

  /**
   * Inicializa o gerenciador Ollama e baixa o modelo padr√£o
   */
  async initialize(): Promise<void> {
    if (this.isInitialized) {
      return;
    }

    try {
      Logger.ollama('üöÄ Inicializando gerenciador Ollama...');
      
      // Verificar se o Ollama est√° instalado
      await this.checkOllamaInstallation();
      
      // Verificar se o servidor est√° rodando
      await this.checkOllamaServer();
      
      // Baixar modelo padr√£o se n√£o existir
      await this.ensureDefaultModel();
      
      this.isInitialized = true;
      Logger.success('‚úÖ Gerenciador Ollama inicializado com modelo padr√£o');
      
    } catch (error) {
      Logger.error('Erro ao inicializar gerenciador Ollama:', error);
      throw error;
    }
  }

  /**
   * Verifica se o Ollama est√° instalado
   */
  private async checkOllamaInstallation(): Promise<void> {
    try {
      const { stdout } = await execAsync('ollama --version');
      Logger.ollama(`‚úÖ Ollama instalado: ${stdout.trim()}`);
    } catch (error) {
      Logger.error('‚ùå Ollama n√£o encontrado!');
      Logger.info('üìã Para instalar o Ollama, visite: https://ollama.ai');
      Logger.info('üíª Ou execute: curl -fsSL https://ollama.ai/install.sh | sh');
      throw new Error('Ollama n√£o est√° instalado. √â um pr√©-requisito para este projeto.');
    }
  }

  /**
   * Verifica se o servidor Ollama est√° rodando
   */
  private async checkOllamaServer(): Promise<void> {
    try {
      // Tentar listar modelos para verificar se o servidor est√° respondendo
      const { stdout } = await execAsync('ollama list');
      Logger.ollama('‚úÖ Servidor Ollama est√° rodando');
    } catch (error) {
      Logger.warn('‚ö†Ô∏è Servidor Ollama n√£o est√° respondendo, verificando status...');
      
      try {
        // Tentar iniciar servidor Ollama
        const { stdout, stderr } = await execAsync('ollama serve', { timeout: 5000 });
        Logger.success('‚úÖ Servidor Ollama iniciado');
        
      } catch (startError: any) {
        // Verificar se o erro √© de porta em uso (Ollama j√° est√° rodando)
        if (startError.stderr && startError.stderr.includes('address already in use')) {
          Logger.ollama('‚úÖ Porta 11434 em uso - Ollama j√° est√° rodando em background');
          // N√£o aguardar, continuar direto - o servidor pode demorar para responder
          return;
        }
        
        // Se n√£o for erro de porta em uso, tentar verificar se o servidor j√° est√° rodando
        try {
          await this.waitForServerReady();
          Logger.ollama('‚úÖ Servidor Ollama j√° estava rodando');
          return;
        } catch (waitError) {
          Logger.error('‚ùå Erro ao iniciar servidor Ollama:', startError);
          throw new Error('Falha ao iniciar servidor Ollama. Execute manualmente: ollama serve');
        }
      }
    }
  }

  /**
   * Aguarda o servidor estar pronto
   */
  private async waitForServerReady(): Promise<void> {
    let attempts = 0;
    const maxAttempts = 30;
    
    while (attempts < maxAttempts) {
      try {
        const { stdout } = await execAsync('ollama list');
        if (stdout && stdout.trim()) {
          return;
        }
      } catch (error) {
        // Servidor ainda n√£o est√° pronto
      }
      
      attempts++;
      await new Promise(resolve => setTimeout(resolve, 1000));
    }
    
    throw new Error('Timeout aguardando servidor Ollama ficar pronto');
  }

  /**
   * Garante que o modelo padr√£o est√° dispon√≠vel
   */
  private async ensureDefaultModel(): Promise<void> {
    try {
      Logger.ollama(`üîç Verificando se modelo padr√£o ${this.defaultModel} est√° dispon√≠vel...`);
      
      const models = await this.listModels();
      Logger.ollama(`üìã Modelos encontrados: ${models.length}`);
      
      if (models.length > 0) {
        Logger.ollama(`üìù Modelos: ${models.map(m => m.name).join(', ')}`);
      }
      
      const defaultModel = models.find(m => m.name === this.defaultModel);
      
      if (defaultModel) {
        Logger.ollama(`‚úÖ Modelo padr√£o ${this.defaultModel} j√° est√° dispon√≠vel`);
        return;
      }
      
      Logger.ollama(`‚ùå Modelo padr√£o ${this.defaultModel} n√£o encontrado. Mostrando seletor...`);
      
      // Perguntar ao usu√°rio se quer baixar o modelo padr√£o
      const shouldDownload = await this.askUserToDownloadDefault();
      
      if (shouldDownload) {
        Logger.ollama(`üì• Baixando modelo padr√£o ${this.defaultModel}...`);
        await this.downloadModelWithProgress(this.defaultModel);
        Logger.success(`‚úÖ Modelo padr√£o ${this.defaultModel} baixado com sucesso`);
      } else {
        Logger.ollama(`ü§ñ Usu√°rio escolheu selecionar outro modelo...`);
        // Permitir que o usu√°rio escolha outro modelo
        const selectedModel = await this.selectModelInteractively();
        if (selectedModel) {
          Logger.ollama(`‚úÖ Usando modelo selecionado: ${selectedModel}`);
          // Atualizar modelo padr√£o
          this.defaultModel = selectedModel;
        } else {
          throw new Error('Nenhum modelo selecionado. √â necess√°rio ter pelo menos um modelo dispon√≠vel.');
        }
      }
      
    } catch (error) {
      Logger.error(`Erro ao garantir modelo padr√£o ${this.defaultModel}:`, error);
      throw error;
    }
  }

  /**
   * Pergunta ao usu√°rio se quer baixar o modelo padr√£o
   */
  private async askUserToDownloadDefault(): Promise<boolean> {
    try {
      const inquirer = (await import('inquirer')).default;
      
      const { action } = await inquirer.prompt([
        {
          type: 'list',
          name: 'action',
          message: `Modelo padr√£o ${this.defaultModel} n√£o encontrado. O que voc√™ gostaria de fazer?`,
          choices: [
            { name: `üì• Baixar ${this.defaultModel} (recomendado)`, value: 'download' },
            { name: 'ü§ñ Escolher outro modelo dispon√≠vel', value: 'select' },
            { name: '‚ùå Cancelar', value: 'cancel' }
          ],
          default: 'download'
        }
      ]);

      if (action === 'cancel') {
        throw new Error('Opera√ß√£o cancelada pelo usu√°rio');
      }

      return action === 'download';
    } catch (error) {
      // Se inquirer falhar, usar fallback
      Logger.info(`Modelo padr√£o ${this.defaultModel} n√£o encontrado.`);
      Logger.info('Para baixar manualmente, execute: ollama pull phi3:mini');
      return false;
    }
  }

  /**
   * Permite ao usu√°rio escolher um modelo interativamente
   */
  private async selectModelInteractively(): Promise<string | null> {
    try {
      const inquirer = (await import('inquirer')).default;
      
      // Listar modelos dispon√≠veis
      const availableModels = await this.listAvailableModels();
      
      if (availableModels.length === 0) {
        Logger.warn('‚ö†Ô∏è Nenhum modelo dispon√≠vel. Baixando modelo padr√£o...');
        await this.downloadModelWithProgress(this.defaultModel);
        return this.defaultModel;
      }

      const { selectedModel } = await inquirer.prompt([
        {
          type: 'list',
          name: 'selectedModel',
          message: 'Escolha um modelo para usar:',
          choices: [
            ...availableModels.map(model => ({
              name: `${model} (recomendado)`,
              value: model
            })),
            { name: 'üì• Baixar novo modelo', value: 'download_new' }
          ]
        }
      ]);

      if (selectedModel === 'download_new') {
        const { newModelName } = await inquirer.prompt([
          {
            type: 'input',
            name: 'newModelName',
            message: 'Digite o nome do modelo para baixar (ex: llama3.2:3b):',
            default: 'llama3.2:3b'
          }
        ]);

        if (newModelName.trim()) {
          Logger.ollama(`üì• Baixando modelo ${newModelName}...`);
          await this.downloadModelWithProgress(newModelName.trim());
          return newModelName.trim();
        }
      }

      return selectedModel;
    } catch (error) {
      Logger.error('Erro ao selecionar modelo:', error);
      return null;
    }
  }

  /**
   * Lista modelos dispon√≠veis para download
   */
  private async listAvailableModels(): Promise<string[]> {
    try {
      // Modelos populares e leves
      return [
        'phi3:mini',
        'llama3.2:3b',
        'mistral:7b',
        'gemma2:2b',
        'qwen2.5:0.5b',
        'codellama:7b'
      ];
    } catch (error) {
      return ['phi3:mini']; // Fallback
    }
  }

  /**
   * Faz download de um modelo com progresso visual
   */
  async downloadModelWithProgress(modelName: string): Promise<void> {
    try {
      Logger.ollama(`üì• Iniciando download do modelo ${modelName}...`);
      
      // Usar spawn para capturar output em tempo real
      const { spawn } = await import('child_process');
      
      return new Promise((resolve, reject) => {
        const ollamaProcess = spawn('ollama', ['pull', modelName]);
        
        let output = '';
        let lastProgress = '';
        
        ollamaProcess.stdout.on('data', (data) => {
          const text = data.toString();
          output += text;
          
          // Mostrar progresso em tempo real
          if (text.includes('pulling')) {
            const progressMatch = text.match(/(\d+\.?\d*)%/);
            if (progressMatch && progressMatch[1] !== lastProgress) {
              lastProgress = progressMatch[1];
              process.stdout.write(`\rüì• Download: ${lastProgress}%`);
            }
          }
        });
        
        ollamaProcess.stderr.on('data', (data) => {
          const text = data.toString();
          if (text.includes('pulling')) {
            // Mostrar progresso do stderr tamb√©m
            const progressMatch = text.match(/(\d+\.?\d*)%/);
            if (progressMatch && progressMatch[1] !== lastProgress) {
              lastProgress = progressMatch[1];
              process.stdout.write(`\rüì• Download: ${lastProgress}%`);
            }
          }
        });
        
        ollamaProcess.on('close', (code) => {
          process.stdout.write('\n'); // Nova linha ap√≥s progresso
          
          if (code === 0) {
            Logger.ollama(`‚úÖ Download do modelo ${modelName} conclu√≠do`);
            // Limpar cache de modelos para refletir a mudan√ßa
            this.modelCache.clear();
            resolve();
          } else {
            reject(new Error(`Download falhou com c√≥digo ${code}`));
          }
        });
        
        ollamaProcess.on('error', (error) => {
          reject(error);
        });
        
        // Timeout de 10 minutos
        setTimeout(() => {
          ollamaProcess.kill();
          reject(new Error('Timeout: download demorou mais de 10 minutos'));
        }, 600000);
      });
      
    } catch (error) {
      Logger.error(`Erro ao fazer download do modelo ${modelName}:`, error);
      throw error;
    }
  }

  /**
   * Gera resposta do modelo com otimiza√ß√µes de performance
   */
  async generateResponse(modelName: string, prompt: string, context?: string): Promise<{ response: string }> {
    // Garantir inicializa√ß√£o
    if (!this.isInitialized) {
      await this.initialize();
    }

    const cacheKey = `${modelName}:${this.hashPrompt(prompt)}`;
    
    // Verificar cache primeiro
    const cached = this.cache.get(cacheKey);
    if (cached && Date.now() - cached.timestamp < cached.ttl) {
      return { response: cached.response };
    }

    try {
      // Verificar se o modelo existe, se n√£o, tentar baixar
      await this.ensureModelAvailable(modelName);
      
      // Garantir que o modelo est√° ativo (ass√≠ncrono)
      this.ensureModelActive(modelName).catch(() => {}); // N√£o bloquear

      // Usar timeout para evitar esperas longas
      const response = await Promise.race([
        this.callOllama(modelName, prompt, context),
        this.timeoutPromise(30000, 'Timeout: resposta demorou mais de 30s')
      ]);

      // Cache da resposta
      this.cache.set(cacheKey, {
        response: response as string,
        timestamp: Date.now(),
        ttl: this.CACHE_TTL
      });

      return { response: response as string };
    } catch (error) {
      Logger.error('Erro ao gerar resposta:', error);
      throw error;
    }
  }

  /**
   * Garante que o modelo est√° dispon√≠vel
   */
  private async ensureModelAvailable(modelName: string): Promise<void> {
    try {
      // Tentar listar modelos com retry
      let models: ModelConfig[] = [];
      let attempts = 0;
      const maxAttempts = 3;
      
      while (attempts < maxAttempts) {
        try {
          models = await this.listModels();
          break;
        } catch (error) {
          attempts++;
          if (attempts >= maxAttempts) {
            throw new Error(`N√£o foi poss√≠vel conectar ao servidor Ollama ap√≥s ${maxAttempts} tentativas`);
          }
          // Aguardar um pouco antes de tentar novamente
          await new Promise(resolve => setTimeout(resolve, 2000));
        }
      }
      
      const model = models.find(m => m.name === modelName);
      
      if (!model) {
        if (modelName === this.defaultModel) {
          // Para o modelo padr√£o, sempre tentar baixar
          await this.downloadModelWithProgress(modelName);
        } else {
          // Para outros modelos, informar que precisa baixar
          throw new Error(`Modelo ${modelName} n√£o encontrado. Use "ollama pull ${modelName}" para baix√°-lo.`);
        }
      }
    } catch (error) {
      Logger.error(`Erro ao verificar modelo ${modelName}:`, error);
      throw error;
    }
  }

  /**
   * Chama Ollama com otimiza√ß√µes
   */
  private async callOllama(modelName: string, prompt: string, context?: string): Promise<string> {
    const fullPrompt = context ? `${context}\n\nPergunta: ${prompt}` : prompt;
    
    const command = `ollama run ${modelName} "${fullPrompt.replace(/"/g, '\\"')}"`;
    
    const { stdout } = await execAsync(command, {
      timeout: 25000, // 25s timeout
      maxBuffer: 1024 * 1024 // 1MB buffer
    });

    return stdout.trim();
  }

  /**
   * Lista modelos com cache otimizado
   */
  async listModels(): Promise<ModelConfig[]> {
    const now = Date.now();
    
    // Verificar cache de modelos
    if (this.modelCache.has('all') && (now - this.lastModelList) < this.MODEL_LIST_CACHE_TTL) {
      return this.modelCache.get('all') || [];
    }

    try {
      Logger.ollama('üìã Listando modelos dispon√≠veis...');
      
      // Usar comando sem --json para compatibilidade com vers√µes antigas
      const { stdout } = await execAsync('ollama list', {
        timeout: 5000, // 5s timeout para listar modelos
        maxBuffer: 1024 * 1024
      });

      // Parsear sa√≠da do comando ollama list
      const models: ModelConfig[] = this.parseOllamaListOutput(stdout);
      
      // Cache dos modelos
      this.modelCache.set('all', models);
      this.lastModelList = now;
      
      Logger.ollama(`‚úÖ ${models.length} modelos encontrados`);
      return models;
      
    } catch (error) {
      Logger.error('Erro ao listar modelos:', error);
      return [];
    }
  }

  /**
   * Parseia a sa√≠da do comando ollama list
   */
  private parseOllamaListOutput(output: string): ModelConfig[] {
    try {
      const lines = output.trim().split('\n');
      const models: ModelConfig[] = [];
      
      // Pular cabe√ßalho se existir
      const startIndex = lines[0].includes('NAME') ? 1 : 0;
      
      for (let i = startIndex; i < lines.length; i++) {
        const line = lines[i].trim();
        if (!line) continue;
        
        // Parsear linha do formato: NAME                TAG                 SIZE       ID
        // Exemplo: phi3:mini         latest            3.8 GB     a1b2c3d4
        const parts = line.split(/\s+/);
        if (parts.length >= 3) {
          const name = parts[0];
          const tag = parts[1];
          const size = parts.slice(2, -1).join(' '); // Tamanho pode ter espa√ßos
          const id = parts[parts.length - 1];
          
          models.push({
            name: name,
            description: `Modelo Ollama: ${name}:${tag}`,
            size: size,
            compatibility: 'Ollama',
            parameters: 0, // N√£o dispon√≠vel na vers√£o antiga
            contextLength: 0, // N√£o dispon√≠vel na vers√£o antiga
            isLocal: true,
            status: 'ready'
          });
        }
      }
      
      return models;
    } catch (error) {
      Logger.warn('Erro ao parsear sa√≠da do ollama list, retornando lista vazia');
      return [];
    }
  }

  /**
   * Garante que o modelo est√° ativo (otimizado)
   */
  async ensureModelActive(modelName: string): Promise<void> {
    try {
      // Verificar se o modelo j√° est√° rodando (sem bloquear)
      const models = await this.listModels();
      const model = models.find(m => m.name === modelName);
      
      if (model && model.status === 'ready') {
        return; // Modelo j√° est√° ativo
      }

      // Iniciar modelo em background
      Logger.ollama(`üöÄ Iniciando modelo ${modelName}...`);
      
      exec(`ollama run ${modelName} "test"`, (error) => {
        if (error) {
          Logger.warn(`Modelo ${modelName} n√£o p√¥de ser iniciado:`, error.message);
        }
      });

      // Aguardar um pouco para o modelo estar pronto
      await this.waitForModelReady(modelName);
      
    } catch (error) {
      Logger.warn(`Erro ao garantir modelo ativo ${modelName}:`, error);
    }
  }

  /**
   * Aguarda modelo estar pronto (com timeout)
   */
  private async waitForModelReady(modelName: string): Promise<void> {
    const maxWait = 10000; // 10s m√°ximo
    const checkInterval = 500; // Verificar a cada 500ms
    
    for (let i = 0; i < maxWait; i += checkInterval) {
      try {
        const models = await this.listModels();
        const model = models.find(m => m.name === modelName);
        
        if (model && model.status === 'ready') {
          return;
        }
        
        await new Promise(resolve => setTimeout(resolve, checkInterval));
      } catch {
        // Continuar tentando
      }
    }
    
    Logger.warn(`Modelo ${modelName} n√£o ficou pronto em ${maxWait}ms`);
  }

  /**
   * Hash simples para cache de prompts
   */
  private hashPrompt(prompt: string): string {
    let hash = 0;
    for (let i = 0; i < prompt.length; i++) {
      const char = prompt.charCodeAt(i);
      hash = ((hash << 5) - hash) + char;
      hash = hash & hash; // Convert to 32bit integer
    }
    return hash.toString();
  }

  /**
   * Promise com timeout
   */
  private timeoutPromise<T>(ms: number, message: string): Promise<T> {
    return new Promise((_, reject) => {
      setTimeout(() => reject(new Error(message)), ms);
    });
  }

  /**
   * Formata tamanho do modelo para exibi√ß√£o
   */
  private formatModelSize(bytes: number): string {
    const sizes = ['B', 'KB', 'MB', 'GB', 'TB'];
    if (bytes === 0) return '0 B';
    
    const i = Math.floor(Math.log(bytes) / Math.log(1024));
    return Math.round(bytes / Math.pow(1024, i) * 100) / 100 + ' ' + sizes[i];
  }

  /**
   * Limpa cache manualmente
   */
  clearCache(): void {
    this.cache.clear();
    this.modelCache.clear();
    Logger.ollama('üóëÔ∏è Cache limpo');
  }

  /**
   * Obt√©m o modelo padr√£o
   */
  getDefaultModel(): string {
    return this.defaultModel;
  }
}
