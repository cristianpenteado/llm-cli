import { exec } from 'child_process';
import { promisify } from 'util';
import { Logger } from '../utils/Logger';
import { ModelConfig } from '../types';

const execAsync = promisify(exec);

export class OllamaManager {
  private cache = new Map<string, { response: string; timestamp: number; ttl: number }>();
  private modelCache = new Map<string, ModelConfig[]>();
  private lastModelList = 0;
  private readonly CACHE_TTL = 30000; // 30 segundos
  private readonly MODEL_LIST_CACHE_TTL = 10000; // 10 segundos
  private readonly DEFAULT_MODEL = 'phi3:mini';
  private isInitialized = false;

  /**
   * Inicializa o gerenciador Ollama e baixa o modelo padr√£o
   */
  async initialize(): Promise<void> {
    if (this.isInitialized) {
      return;
    }

    try {
      Logger.ollama('üöÄ Inicializando gerenciador Ollama...');
      
      // Verificar se o Ollama est√° instalado
      await this.checkOllamaInstallation();
      
      // Verificar se o servidor est√° rodando
      await this.checkOllamaServer();
      
      // Baixar modelo padr√£o se n√£o existir
      await this.ensureDefaultModel();
      
      this.isInitialized = true;
      Logger.success('‚úÖ Gerenciador Ollama inicializado com modelo padr√£o');
      
    } catch (error) {
      Logger.error('Erro ao inicializar gerenciador Ollama:', error);
      throw error;
    }
  }

  /**
   * Verifica se o Ollama est√° instalado
   */
  private async checkOllamaInstallation(): Promise<void> {
    try {
      const { stdout } = await execAsync('ollama --version');
      Logger.ollama(`‚úÖ Ollama instalado: ${stdout.trim()}`);
    } catch (error) {
      Logger.error('‚ùå Ollama n√£o encontrado!');
      Logger.info('üìã Para instalar o Ollama, visite: https://ollama.ai');
      Logger.info('üíª Ou execute: curl -fsSL https://ollama.ai/install.sh | sh');
      throw new Error('Ollama n√£o est√° instalado. √â um pr√©-requisito para este projeto.');
    }
  }

  /**
   * Verifica se o servidor Ollama est√° rodando
   */
  private async checkOllamaServer(): Promise<void> {
    try {
      const { stdout } = await execAsync('ollama list --json');
      Logger.ollama('‚úÖ Servidor Ollama est√° rodando');
    } catch (error) {
      Logger.warn('‚ö†Ô∏è Servidor Ollama n√£o est√° rodando');
      Logger.info('üöÄ Iniciando servidor Ollama...');
      
      try {
        // Iniciar servidor Ollama em background
        exec('ollama serve', (error) => {
          if (error) {
            Logger.warn('‚ö†Ô∏è Erro ao iniciar servidor Ollama:', error.message);
          }
        });
        
        // Aguardar servidor estar pronto
        await this.waitForServerReady();
        Logger.success('‚úÖ Servidor Ollama iniciado');
        
      } catch (startError) {
        Logger.error('‚ùå Erro ao iniciar servidor Ollama:', startError);
        throw new Error('Falha ao iniciar servidor Ollama. Execute manualmente: ollama serve');
      }
    }
  }

  /**
   * Aguarda o servidor estar pronto
   */
  private async waitForServerReady(): Promise<void> {
    let attempts = 0;
    const maxAttempts = 30;
    
    while (attempts < maxAttempts) {
      try {
        const { stdout } = await execAsync('ollama list --json');
        if (stdout) {
          return;
        }
      } catch (error) {
        // Servidor ainda n√£o est√° pronto
      }
      
      attempts++;
      await new Promise(resolve => setTimeout(resolve, 1000));
    }
    
    throw new Error('Timeout aguardando servidor Ollama ficar pronto');
  }

  /**
   * Garante que o modelo padr√£o est√° dispon√≠vel
   */
  private async ensureDefaultModel(): Promise<void> {
    try {
      const models = await this.listModels();
      const defaultModel = models.find(m => m.name === this.DEFAULT_MODEL);
      
      if (defaultModel) {
        Logger.ollama(`‚úÖ Modelo padr√£o ${this.DEFAULT_MODEL} j√° est√° dispon√≠vel`);
        return;
      }
      
      Logger.ollama(`üì• Baixando modelo padr√£o ${this.DEFAULT_MODEL}...`);
      await this.downloadModel(this.DEFAULT_MODEL);
      Logger.success(`‚úÖ Modelo padr√£o ${this.DEFAULT_MODEL} baixado com sucesso`);
      
    } catch (error) {
      Logger.error(`Erro ao baixar modelo padr√£o ${this.DEFAULT_MODEL}:`, error);
      throw error;
    }
  }

  /**
   * Faz download de um modelo
   */
  async downloadModel(modelName: string): Promise<void> {
    try {
      Logger.ollama(`üì• Fazendo download do modelo ${modelName}...`);
      
      // Usar o comando ollama pull com timeout
      const result = await Promise.race([
        execAsync(`ollama pull ${modelName}`),
        this.timeoutPromise(300000, `Timeout: download do modelo ${modelName} demorou mais de 5 minutos`) // 5 min
      ]) as { stdout: string; stderr: string };
      
      if (result.stderr && !result.stderr.includes('pulling')) {
        throw new Error(result.stderr);
      }
      
      Logger.ollama(`‚úÖ Download do modelo ${modelName} conclu√≠do`);
      
      // Limpar cache de modelos para refletir a mudan√ßa
      this.modelCache.clear();
      
    } catch (error) {
      Logger.error(`Erro ao fazer download do modelo ${modelName}:`, error);
      throw error;
    }
  }

  /**
   * Gera resposta do modelo com otimiza√ß√µes de performance
   */
  async generateResponse(modelName: string, prompt: string, context?: string): Promise<{ response: string }> {
    // Garantir inicializa√ß√£o
    if (!this.isInitialized) {
      await this.initialize();
    }

    const cacheKey = `${modelName}:${this.hashPrompt(prompt)}`;
    
    // Verificar cache primeiro
    const cached = this.cache.get(cacheKey);
    if (cached && Date.now() - cached.timestamp < cached.ttl) {
      Logger.ollama('‚ö° Resposta retornada do cache');
      return { response: cached.response };
    }

    try {
      // Verificar se o modelo existe, se n√£o, tentar baixar
      await this.ensureModelAvailable(modelName);
      
      // Garantir que o modelo est√° ativo (ass√≠ncrono)
      this.ensureModelActive(modelName).catch(() => {}); // N√£o bloquear

      Logger.ollama(`üí¨ Gerando resposta do modelo: ${modelName}`);
      
      // Usar timeout para evitar esperas longas
      const response = await Promise.race([
        this.callOllama(modelName, prompt, context),
        this.timeoutPromise(30000, 'Timeout: resposta demorou mais de 30s')
      ]);

      // Cache da resposta
      this.cache.set(cacheKey, {
        response: response as string,
        timestamp: Date.now(),
        ttl: this.CACHE_TTL
      });

      return { response: response as string };
    } catch (error) {
      Logger.error('Erro ao gerar resposta:', error);
      throw error;
    }
  }

  /**
   * Garante que o modelo est√° dispon√≠vel
   */
  private async ensureModelAvailable(modelName: string): Promise<void> {
    const models = await this.listModels();
    const model = models.find(m => m.name === modelName);
    
    if (!model) {
      if (modelName === this.DEFAULT_MODEL) {
        // Para o modelo padr√£o, sempre tentar baixar
        await this.downloadModel(modelName);
      } else {
        // Para outros modelos, informar que precisa baixar
        throw new Error(`Modelo ${modelName} n√£o encontrado. Use "ollama pull ${modelName}" para baix√°-lo.`);
      }
    }
  }

  /**
   * Chama Ollama com otimiza√ß√µes
   */
  private async callOllama(modelName: string, prompt: string, context?: string): Promise<string> {
    const fullPrompt = context ? `${context}\n\nPergunta: ${prompt}` : prompt;
    
    const command = `ollama run ${modelName} "${fullPrompt.replace(/"/g, '\\"')}"`;
    
    const { stdout } = await execAsync(command, {
      timeout: 25000, // 25s timeout
      maxBuffer: 1024 * 1024 // 1MB buffer
    });

    return stdout.trim();
  }

  /**
   * Lista modelos com cache otimizado
   */
  async listModels(): Promise<ModelConfig[]> {
    const now = Date.now();
    
    // Verificar cache de modelos
    if (this.modelCache.has('all') && (now - this.lastModelList) < this.MODEL_LIST_CACHE_TTL) {
      return this.modelCache.get('all') || [];
    }

    try {
      Logger.ollama('üìã Listando modelos dispon√≠veis...');
      
      const { stdout } = await execAsync('ollama list --json', {
        timeout: 5000, // 5s timeout para listar modelos
        maxBuffer: 1024 * 1024
      });

      const rawModels = JSON.parse(stdout);
      const models: ModelConfig[] = rawModels.map((model: any) => ({
        name: model.name,
        description: `Modelo Ollama: ${model.name}`,
        size: this.formatModelSize(model.size || 0),
        compatibility: 'Ollama',
        parameters: model.parameter_size || 0,
        contextLength: model.context_length || 0,
        isLocal: true,
        status: 'ready'
      }));
      
      // Cache dos modelos
      this.modelCache.set('all', models);
      this.lastModelList = now;
      
      Logger.ollama(`‚úÖ ${models.length} modelos encontrados`);
      return models;
      
    } catch (error) {
      Logger.error('Erro ao listar modelos:', error);
      return [];
    }
  }

  /**
   * Garante que o modelo est√° ativo (otimizado)
   */
  async ensureModelActive(modelName: string): Promise<void> {
    try {
      // Verificar se o modelo j√° est√° rodando (sem bloquear)
      const models = await this.listModels();
      const model = models.find(m => m.name === modelName);
      
      if (model && model.status === 'ready') {
        return; // Modelo j√° est√° ativo
      }

      // Iniciar modelo em background
      Logger.ollama(`üöÄ Iniciando modelo ${modelName}...`);
      
      exec(`ollama run ${modelName} "test"`, (error) => {
        if (error) {
          Logger.warn(`Modelo ${modelName} n√£o p√¥de ser iniciado:`, error.message);
        }
      });

      // Aguardar um pouco para o modelo estar pronto
      await this.waitForModelReady(modelName);
      
    } catch (error) {
      Logger.warn(`Erro ao garantir modelo ativo ${modelName}:`, error);
    }
  }

  /**
   * Aguarda modelo estar pronto (com timeout)
   */
  private async waitForModelReady(modelName: string): Promise<void> {
    const maxWait = 10000; // 10s m√°ximo
    const checkInterval = 500; // Verificar a cada 500ms
    
    for (let i = 0; i < maxWait; i += checkInterval) {
      try {
        const models = await this.listModels();
        const model = models.find(m => m.name === modelName);
        
        if (model && model.status === 'ready') {
          return;
        }
        
        await new Promise(resolve => setTimeout(resolve, checkInterval));
      } catch {
        // Continuar tentando
      }
    }
    
    Logger.warn(`Modelo ${modelName} n√£o ficou pronto em ${maxWait}ms`);
  }

  /**
   * Hash simples para cache de prompts
   */
  private hashPrompt(prompt: string): string {
    let hash = 0;
    for (let i = 0; i < prompt.length; i++) {
      const char = prompt.charCodeAt(i);
      hash = ((hash << 5) - hash) + char;
      hash = hash & hash; // Convert to 32bit integer
    }
    return hash.toString();
  }

  /**
   * Promise com timeout
   */
  private timeoutPromise<T>(ms: number, message: string): Promise<T> {
    return new Promise((_, reject) => {
      setTimeout(() => reject(new Error(message)), ms);
    });
  }

  /**
   * Formata tamanho do modelo para exibi√ß√£o
   */
  private formatModelSize(bytes: number): string {
    const sizes = ['B', 'KB', 'MB', 'GB', 'TB'];
    if (bytes === 0) return '0 B';
    
    const i = Math.floor(Math.log(bytes) / Math.log(1024));
    return Math.round(bytes / Math.pow(1024, i) * 100) / 100 + ' ' + sizes[i];
  }

  /**
   * Limpa cache manualmente
   */
  clearCache(): void {
    this.cache.clear();
    this.modelCache.clear();
    Logger.ollama('üóëÔ∏è Cache limpo');
  }

  /**
   * Obt√©m o modelo padr√£o
   */
  getDefaultModel(): string {
    return this.DEFAULT_MODEL;
  }
}
