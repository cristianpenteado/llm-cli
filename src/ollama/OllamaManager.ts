import { exec, spawn } from 'child_process';
import { promisify } from 'util';
import { OllamaConfig, ModelConfig } from '../types';
import { Logger } from '../utils/Logger';
import { ConfigManager } from '../utils/ConfigManager';
import * as fs from 'fs-extra';
import * as path from 'path';

const execAsync = promisify(exec);

export class OllamaManager {
  private config: OllamaConfig;
  private configManager: ConfigManager;
  private isConnected: boolean = false;

  constructor() {
    this.configManager = new ConfigManager();
    this.config = this.configManager.getOllamaConfig();
  }

  /**
   * Inicializa o gerenciador Ollama
   */
  async initialize(): Promise<void> {
    try {
      Logger.ollama('üöÄ Inicializando gerenciador Ollama...');
      
      // Verificar se o Ollama est√° instalado
      await this.checkOllamaInstallation();
      
      // Verificar se o servidor Ollama est√° rodando
      await this.checkOllamaServer();
      
      // Conectar ao servidor
      await this.connect();
      
      Logger.success('‚úÖ Gerenciador Ollama inicializado');
      
    } catch (error) {
      Logger.error('Erro ao inicializar gerenciador Ollama:', error);
      throw error;
    }
  }

  /**
   * Verifica se o Ollama est√° instalado
   */
  private async checkOllamaInstallation(): Promise<void> {
    try {
      const { stdout } = await execAsync('ollama --version');
      Logger.ollama(`‚úÖ Ollama instalado: ${stdout.trim()}`);
    } catch (error) {
      Logger.error('‚ùå Ollama n√£o encontrado!');
      Logger.info('üìã Para instalar o Ollama, visite: https://ollama.ai');
      Logger.info('üíª Ou execute: curl -fsSL https://ollama.ai/install.sh | sh');
      throw new Error('Ollama n√£o est√° instalado. √â um pr√©-requisito para este projeto.');
    }
  }

  /**
   * Verifica se o servidor Ollama est√° rodando
   */
  private async checkOllamaServer(): Promise<void> {
    try {
      const { stdout } = await execAsync(`curl -s http://${this.config.serverUrl}:${this.config.port}/api/tags`);
      Logger.ollama('‚úÖ Servidor Ollama est√° rodando');
    } catch (error) {
      Logger.warn('‚ö†Ô∏è Servidor Ollama n√£o est√° rodando');
      Logger.info('üöÄ Iniciando servidor Ollama...');
      
      try {
        // Iniciar servidor Ollama em background
        spawn('ollama', ['serve'], {
          detached: true,
          stdio: 'ignore'
        });
        
        // Aguardar servidor estar pronto
        await this.waitForServerReady();
        Logger.success('‚úÖ Servidor Ollama iniciado');
        
      } catch (startError) {
        Logger.error('‚ùå Erro ao iniciar servidor Ollama:', startError);
        throw new Error('Falha ao iniciar servidor Ollama. Execute manualmente: ollama serve');
      }
    }
  }

  /**
   * Aguarda o servidor estar pronto
   */
  private async waitForServerReady(): Promise<void> {
    let attempts = 0;
    const maxAttempts = 30;
    
    while (attempts < maxAttempts) {
      try {
        const { stdout } = await execAsync(`curl -s http://${this.config.serverUrl}:${this.config.port}/api/tags`);
        if (stdout) {
          return;
        }
      } catch (error) {
        // Servidor ainda n√£o est√° pronto
      }
      
      attempts++;
      await new Promise(resolve => setTimeout(resolve, 1000));
    }
    
    throw new Error('Timeout aguardando servidor Ollama ficar pronto');
  }

  /**
   * Aguarda um modelo ficar pronto
   */
  private async waitForModelReady(modelName: string): Promise<void> {
    let attempts = 0;
    const maxAttempts = 60;
    
    while (attempts < maxAttempts) {
      try {
        const models = await this.listModels();
        const model = models.find(m => m.name === modelName);
        
        if (model && model.status === 'ready') {
          return;
        }
      } catch (error) {
        // Modelo ainda n√£o est√° pronto
      }
      
      attempts++;
      await new Promise(resolve => setTimeout(resolve, 1000));
    }
    
    throw new Error(`Timeout aguardando modelo ${modelName} ficar pronto`);
  }

  /**
   * Conecta ao servidor Ollama
   */
  async connect(): Promise<void> {
    try {
      Logger.ollama('üîå Conectando ao servidor Ollama...');
      
      // Testar conex√£o usando a API de tags
      const { stdout } = await execAsync(`curl -s http://${this.config.serverUrl}:${this.config.port}/api/tags`);
      
      if (stdout) {
        this.isConnected = true;
        Logger.success('‚úÖ Conectado ao servidor Ollama');
      } else {
        throw new Error('Servidor Ollama n√£o respondeu corretamente');
      }
      
    } catch (error) {
      Logger.error('Erro ao conectar ao servidor Ollama:', error);
      this.isConnected = false;
      throw new Error('Falha ao conectar ao servidor Ollama');
    }
  }

  /**
   * Verifica se est√° conectado
   */
  isConnectedToServer(): boolean {
    return this.isConnected;
  }

  /**
   * Lista modelos dispon√≠veis
   */
  async listModels(): Promise<ModelConfig[]> {
    if (!this.isConnected) {
      await this.connect();
    }

    try {
      Logger.ollama('üìã Listando modelos dispon√≠veis...');
      
      const { stdout } = await execAsync(`curl -s http://${this.config.serverUrl}:${this.config.port}/api/tags`);
      const response = JSON.parse(stdout);
      
      if (!response.models) {
        Logger.warn('‚ö†Ô∏è Nenhum modelo encontrado');
        return [];
      }
      
      const models: ModelConfig[] = response.models.map((model: any) => ({
        name: model.name,
        description: `Modelo Ollama: ${model.name}`,
        size: this.formatModelSize(model.size || 0),
        compatibility: 'Ollama',
        parameters: model.parameter_size || 0,
        contextLength: model.context_length || 0,
        isLocal: true,
        status: 'ready'
      }));
      
      Logger.ollama(`‚úÖ ${models.length} modelos encontrados`);
      return models;
      
    } catch (error) {
      Logger.error('Erro ao listar modelos:', error);
      return [];
    }
  }

  /**
   * Obt√©m informa√ß√µes detalhadas de um modelo
   */
  async getModelInfo(modelName: string): Promise<ModelConfig | null> {
    try {
      const models = await this.listModels();
      return models.find(m => m.name === modelName) || null;
    } catch (error) {
      Logger.error('Erro ao obter informa√ß√µes do modelo:', error);
      return null;
    }
  }

  /**
   * Gera resposta de um modelo
   */
  async generateResponse(modelName: string, prompt: string, context?: string): Promise<{ response: string }> {
    try {
      Logger.ollama(`üí¨ Gerando resposta do modelo: ${modelName}`);
      
      // Garantir que o modelo est√° ativo
      await this.ensureModelActive(modelName);
      
      // Preparar prompt com contexto
      const fullPrompt = context ? `${context}\n\nUsu√°rio: ${prompt}\nAssistente:` : prompt;
      
      // Enviar prompt para o Ollama
      const { stdout } = await execAsync(`echo '${fullPrompt.replace(/'/g, "'\"'\"'")}' | ollama run ${modelName}`);
      
      Logger.success(`‚úÖ Resposta gerada do modelo ${modelName}`);
      
      return {
        response: stdout.trim()
      };
      
    } catch (error) {
      Logger.error('Erro ao gerar resposta do modelo:', error);
      throw error;
    }
  }

  /**
   * Garante que o modelo est√° ativo
   */
  async ensureModelActive(modelName: string): Promise<void> {
    try {
      const models = await this.listModels();
      const model = models.find(m => m.name === modelName);
      
      if (!model) {
        throw new Error(`Modelo ${modelName} n√£o encontrado`);
      }
      
      if (model.status !== 'ready') {
        Logger.ollama(`üöÄ Iniciando modelo: ${modelName}`);
        await this.startModel(modelName);
        
        // Aguardar modelo ficar pronto
        await this.waitForModelReady(modelName);
      }
      
    } catch (error) {
      Logger.error('Erro ao garantir modelo ativo:', error);
      throw error;
    }
  }

  /**
   * Inicia um modelo
   */
  async startModel(modelName: string): Promise<void> {
    if (!this.isConnected) {
      await this.connect();
    }

    try {
      Logger.ollama(`üöÄ Iniciando modelo ${modelName}...`);
      
      // Para Ollama, os modelos s√£o carregados automaticamente quando solicitados
      // Vamos apenas verificar se o modelo est√° dispon√≠vel
      const modelInfo = await this.getModelInfo(modelName);
      if (!modelInfo) {
        throw new Error(`Modelo ${modelName} n√£o encontrado`);
      }
      
      Logger.ollama(`‚úÖ Modelo ${modelName} est√° pronto para uso`);
      
    } catch (error) {
      Logger.error('Erro ao iniciar modelo:', error);
      throw error;
    }
  }

  /**
   * Para um modelo
   */
  async stopModel(modelName: string): Promise<void> {
    if (!this.isConnected) {
      await this.connect();
    }

    try {
      Logger.ollama(`üõë Parando modelo ${modelName}...`);
      
      // Ollama gerencia a mem√≥ria automaticamente, n√£o h√° necessidade de parar modelos
      Logger.ollama(`‚úÖ Modelo ${modelName} foi liberado da mem√≥ria`);
      
    } catch (error) {
      Logger.error('Erro ao parar modelo:', error);
      throw error;
    }
  }

  /**
   * Obt√©m status de um modelo
   */
  async getModelStatus(modelName: string): Promise<string> {
    if (!this.isConnected) {
      await this.connect();
    }

    try {
      Logger.ollama(`üìä Verificando status do modelo ${modelName}...`);
      
      const modelInfo = await this.getModelInfo(modelName);
      if (modelInfo) {
        Logger.ollama(`‚úÖ Status do modelo ${modelName}: ready`);
        return 'ready';
      } else {
        Logger.ollama(`‚ùå Status do modelo ${modelName}: not_found`);
        return 'not_found';
      }
      
    } catch (error) {
      Logger.error('Erro ao obter status do modelo:', error);
      return 'error';
    }
  }

  /**
   * Faz download de um modelo
   */
  async downloadModel(modelName: string): Promise<void> {
    if (!this.isConnected) {
      await this.connect();
    }

    try {
      Logger.ollama(`üì• Fazendo download do modelo ${modelName}...`);
      
      // Usar o comando ollama pull
      const { stdout, stderr } = await execAsync(`ollama pull ${modelName}`);
      
      if (stderr && !stderr.includes('pulling')) {
        throw new Error(stderr);
      }
      
      Logger.ollama(`‚úÖ Download do modelo ${modelName} conclu√≠do`);
      
    } catch (error) {
      Logger.error('Erro ao fazer download do modelo:', error);
      throw error;
    }
  }

  /**
   * Remove um modelo
   */
  async removeModel(modelName: string): Promise<void> {
    if (!this.isConnected) {
      await this.connect();
    }

    try {
      Logger.ollama(`üóëÔ∏è Removendo modelo ${modelName}...`);
      
      // Usar o comando ollama rm
      await execAsync(`ollama rm ${modelName}`);
      
      Logger.ollama(`‚úÖ Modelo ${modelName} removido`);
      
    } catch (error) {
      Logger.error('Erro ao remover modelo:', error);
      throw error;
    }
  }

  /**
   * Obt√©m estat√≠sticas do servidor
   */
  async getServerStats(): Promise<any> {
    if (!this.isConnected) {
      await this.connect();
    }

    try {
      Logger.ollama('üìä Obtendo estat√≠sticas do servidor...');
      
      // Ollama n√£o tem endpoint de estat√≠sticas, vamos retornar informa√ß√µes b√°sicas
      const stats = {
        serverUrl: this.config.serverUrl,
        port: this.config.port,
        isConnected: this.isConnected,
        modelsPath: this.config.modelsPath
      };
      
      Logger.ollama('‚úÖ Estat√≠sticas do servidor obtidas');
      return stats;
      
    } catch (error) {
      Logger.error('Erro ao obter estat√≠sticas do servidor:', error);
      return null;
    }
  }

  /**
   * Para o servidor Ollama
   */
  async shutdown(): Promise<void> {
    try {
      Logger.ollama('üõë Parando servidor Ollama...');
      
      // Ollama n√£o tem endpoint de shutdown via API, o usu√°rio deve parar manualmente
      Logger.warn('‚ö†Ô∏è Para parar o servidor Ollama, execute: pkill ollama');
      
      this.isConnected = false;
      Logger.success('‚úÖ Gerenciador Ollama desconectado');
      
    } catch (error) {
      Logger.error('Erro ao parar servidor Ollama:', error);
      throw error;
    }
  }

  /**
   * Formata tamanho do modelo para exibi√ß√£o
   */
  private formatModelSize(bytes: number): string {
    const sizes = ['B', 'KB', 'MB', 'GB', 'TB'];
    if (bytes === 0) return '0 B';
    
    const i = Math.floor(Math.log(bytes) / Math.log(1024));
    return Math.round(bytes / Math.pow(1024, i) * 100) / 100 + ' ' + sizes[i];
  }

  /**
   * Verifica se o servidor est√° rodando e pronto
   */
  async isServerReady(): Promise<boolean> {
    try {
      const { stdout } = await execAsync(`curl -s http://${this.config.serverUrl}:${this.config.port}/api/tags`);
      return !!stdout;
    } catch {
      return false;
    }
  }

  /**
   * Obt√©m estat√≠sticas de uso
   */
  getOllamaStats(): any {
    return {
      isConnected: this.isConnected,
      serverUrl: this.config.serverUrl,
      port: this.config.port,
      modelsPath: this.config.modelsPath
    };
  }
}
