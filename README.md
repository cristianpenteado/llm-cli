# LLM CLI

> **AI Agent Terminal** - Desenvolvido para a comunidade â¤ï¸

Uma CLI inteligente que funciona como um agente de IA local, integrando-se diretamente com modelos LLM locais via Ollama. A CLI agora inclui um servidor MCP integrado e otimizaÃ§Ãµes de performance para respostas ultra-rÃ¡pidas.

## ğŸš€ CaracterÃ­sticas Principais

### âœ¨ **Performance Otimizada**
- **Cache inteligente** de respostas e modelos para respostas instantÃ¢neas
- **Timeouts configurÃ¡veis** para evitar esperas longas
- **Processamento assÃ­ncrono** para melhor responsividade
- **Fallback automÃ¡tico** entre MCP e Ollama direto

### ğŸ¤– **Modelo PadrÃ£o AutomÃ¡tico**
- **Download automÃ¡tico** do `phi3:mini` na primeira inicializaÃ§Ã£o
- **VerificaÃ§Ã£o inteligente** de modelos disponÃ­veis
- **Fallback automÃ¡tico** para download quando necessÃ¡rio
- **InicializaÃ§Ã£o automÃ¡tica** do OllamaManager

### ğŸ”Œ **Arquitetura Integrada**
- **Servidor MCP integrado** (nÃ£o depende de processos externos)
- **ComunicaÃ§Ã£o direta** com Ollama para mÃ¡xima velocidade
- **Cache em memÃ³ria** para operaÃ§Ãµes repetidas
- **OtimizaÃ§Ãµes de rede** para modelos locais

### ğŸ¯ **Funcionalidades Inteligentes**
- **DetecÃ§Ã£o automÃ¡tica** de linguagem e framework do projeto
- **Contexto inteligente** baseado na estrutura do projeto
- **SugestÃµes contextuais** para melhor produtividade
- **Interface conversacional** natural e intuitiva

## ğŸ“¦ InstalaÃ§Ã£o

### PrÃ©-requisitos
- **Node.js** 18+ 
- **Ollama** instalado e configurado

### Instalar CLI
```bash
# Instalar globalmente
npm install -g llm-cli

# Ou usar npx
npx llm-cli
```

### Instalar Ollama
```bash
# Linux/macOS
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# Baixar de https://ollama.ai/download
```

## ğŸš€ Uso RÃ¡pido

### 1. **Inicializar Projeto** (Download automÃ¡tico do phi3:mini)
```bash
cd seu-projeto
llm init
# âœ… phi3:mini serÃ¡ baixado automaticamente se nÃ£o existir
```

### 2. **Iniciar Chat** (Respostas ultra-rÃ¡pidas)
```bash
llm chat
# âš¡ Cache inteligente para respostas instantÃ¢neas
# ğŸ”„ Fallback automÃ¡tico se MCP falhar
```

### 3. **Comandos Principais**
```bash
llm init          # Inicializar projeto (baixa modelo padrÃ£o)
llm chat          # Chat conversacional otimizado
llm status        # Status do projeto e modelo
llm list-models   # Listar modelos disponÃ­veis
llm change-model  # Trocar modelo do projeto
```

## ğŸ¯ Casos de Uso

### **Desenvolvedores**
- **Code Review** rÃ¡pido com contexto do projeto
- **Debugging** assistido por IA
- **RefatoraÃ§Ã£o** inteligente de cÃ³digo
- **DocumentaÃ§Ã£o** automÃ¡tica

### **Arquitetos**
- **AnÃ¡lise** de estrutura de projetos
- **RecomendaÃ§Ãµes** de padrÃµes
- **OtimizaÃ§Ãµes** de performance
- **MigraÃ§Ãµes** assistidas

### **DevOps**
- **AnÃ¡lise** de logs e mÃ©tricas
- **AutomaÃ§Ã£o** de processos
- **Troubleshooting** inteligente
- **Monitoramento** proativo

## âš¡ OtimizaÃ§Ãµes de Performance

### **Cache Inteligente**
- **Respostas em cache** por 30 segundos
- **Modelos em cache** por 10 segundos
- **Hash de prompts** para identificaÃ§Ã£o Ãºnica
- **Limpeza automÃ¡tica** de cache expirado

### **Timeouts ConfigurÃ¡veis**
- **Resposta do modelo**: 30 segundos
- **Download de modelo**: 5 minutos
- **Listagem de modelos**: 5 segundos
- **InicializaÃ§Ã£o**: 10 segundos

### **Processamento AssÃ­ncrono**
- **Modelos em background** para nÃ£o bloquear
- **Cache limpo** em paralelo
- **VerificaÃ§Ãµes nÃ£o-bloqueantes** de status
- **Fallbacks automÃ¡ticos** para melhor UX

## ğŸ”§ ConfiguraÃ§Ã£o

### **Modelo PadrÃ£o**
```bash
# O phi3:mini Ã© baixado automaticamente
# Para usar outros modelos:
ollama pull nome-do-modelo
llm change-model nome-do-modelo
```

### **Cache e Performance**
```bash
# Limpar cache manualmente
llm clear-cache

# Ver estatÃ­sticas
llm status
```

## ğŸ—ï¸ Arquitetura

### **Componentes Principais**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLMCLI        â”‚    â”‚  OllamaManager  â”‚    â”‚   MCPServer     â”‚
â”‚   (Orquestrador)â”‚â—„â”€â”€â–ºâ”‚  (Cache + MCP)  â”‚â—„â”€â”€â–ºâ”‚  (Integrado)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ProjectManager   â”‚    â”‚  Conversation   â”‚    â”‚   FileManager   â”‚
â”‚(Projetos)      â”‚    â”‚  Manager        â”‚    â”‚  (Arquivos)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Fluxo de Performance**
1. **Cache Check** â†’ Resposta instantÃ¢nea se disponÃ­vel
2. **MCP Integrado** â†’ ComunicaÃ§Ã£o direta sem processos externos
3. **Ollama Fallback** â†’ Resposta direta se MCP falhar
4. **Cache Update** â†’ Armazenar para futuras consultas

## ğŸš€ Modelos Recomendados

### **Modelo PadrÃ£o (AutomÃ¡tico)**
- **`phi3:mini`** - Leve, rÃ¡pido, ideal para desenvolvimento

### **Modelos Adicionais**
- **`llama3.2:3b`** - Equilibrado entre velocidade e qualidade
- **`mistral:7b`** - Excelente para cÃ³digo e documentaÃ§Ã£o
- **`codellama:7b`** - Especializado em desenvolvimento

> **ğŸ’¡ Dica**: O `phi3:mini` Ã© baixado automaticamente. Para outros modelos, use `ollama pull nome-do-modelo`

## ğŸ” Troubleshooting

### **Modelo nÃ£o encontrado**
```bash
# Baixar manualmente
ollama pull nome-do-modelo

# Ou usar modelo padrÃ£o
llm change-model phi3:mini
```

### **Performance lenta**
```bash
# Limpar cache
llm clear-cache

# Verificar status
llm status
```

### **Erro de conexÃ£o**
```bash
# Verificar Ollama
ollama list

# Reiniciar servidor
ollama serve
```

## ğŸ¤ ContribuiÃ§Ã£o

### **Como Contribuir**
1. **Fork** o projeto
2. **Crie** uma branch para sua feature
3. **Commit** suas mudanÃ§as
4. **Push** para a branch
5. **Abra** um Pull Request

### **Ãreas de Melhoria**
- **Novos modelos** de IA
- **OtimizaÃ§Ãµes** de performance
- **IntegraÃ§Ãµes** com outras ferramentas
- **Testes** e documentaÃ§Ã£o

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

## ğŸ™ Agradecimentos

- **Ollama** pela plataforma de modelos locais
- **Comunidade open-source** pelo suporte contÃ­nuo
- **Contribuidores** que tornaram este projeto possÃ­vel

---

**â­ Se este projeto te ajudou, considere dar uma estrela!**

**ğŸ’¬ DÃºvidas? Abra uma issue ou participe das discussÃµes!**

**ğŸš€ Desenvolvido para a comunidade â¤ï¸**
