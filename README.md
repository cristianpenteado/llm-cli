# ğŸš€ **LLM CLI - AI Agent Terminal**

> **CLI inteligente para desenvolvimento com modelos LLMs locais**  
> **Desenvolvido para a comunidade â¤ï¸**

## ğŸŒŸ **Principais CaracterÃ­sticas**

- **ğŸ¤– AI Agent Local**: Funciona offline com modelos locais via Ollama
- **ğŸ¨ Interface Elegante**: Banner LLM CLI em degradÃª roxo com ASCII art
- **ğŸ”„ MCP Integrado**: Servidor MCP interno com fallback para Ollama
- **ğŸ“ InicializaÃ§Ã£o Inteligente**: Detecta linguagem/framework automaticamente
- **ğŸ’¬ Chat Interativo**: Interface conversacional com comandos especiais
- **âš¡ SeleÃ§Ã£o Intuitiva**: NavegaÃ§Ã£o com setas e confirmaÃ§Ãµes visuais
- **ğŸ”’ Privacidade Total**: Tudo roda localmente, sem envio de dados

## ğŸ® **Como Funciona**

### 1. **Primeira ExecuÃ§Ã£o - ConfiguraÃ§Ã£o Inicial**
```bash
# A CLI oferece modelos recomendados automaticamente
llm init

# Recomenda modelos baseados em:
# - Tamanho e eficiÃªncia
# - EspecializaÃ§Ã£o (cÃ³digo, geral, etc.)
# - Compatibilidade com diferentes sistemas
```

### 2. **InicializaÃ§Ã£o de Projeto - Modelo AutomÃ¡tico**
```bash
# Em qualquer pasta de projeto
llm init

# A CLI:
# âœ… Detecta linguagem/framework automaticamente
# ğŸ¤– Sugere modelos recomendados
# ğŸ“ Cria estrutura de projeto com contexto
# ğŸ”§ Configura modelo padrÃ£o
# ğŸ“ Atualiza .gitignore automaticamente
```

### 3. **Chat Inteligente com Fallback**
```bash
# Iniciar conversa
llm chat

# Se o projeto nÃ£o estiver inicializado:
# âš ï¸ "Projeto nÃ£o inicializado! Deseja inicializar agora? (S/n)"
# ğŸš€ Inicializa automaticamente se confirmado
# ğŸ’¬ Inicia chat com modelo configurado

# Fallback automÃ¡tico:
# ğŸ”Œ Tenta conectar via MCP integrado
# âš ï¸ Se falhar, usa Ollama diretamente
# âœ… Chat funciona independente do protocolo
```

## âš¡ **Modelos Recomendados**

### **Modelos Leves e Eficientes**
- `phi3:mini` - 3.8B parÃ¢metros, rÃ¡pido e eficiente
- `gemma2:2b` - 2B parÃ¢metros, muito leve
- `mistral:7b-instruct` - 7B parÃ¢metros, equilibrado

### **Modelos Especializados em CÃ³digo**
- `deepseek-coder:6.7b-instruct` - Excelente para desenvolvimento
- `codellama:7b-instruct` - Especializado em cÃ³digo da Meta
- `codegemma:7b` - Foco em programaÃ§Ã£o

### **Modelos Gerais de Alta Qualidade**
- `llama3.1:8b-instruct` - Qualidade superior, versÃ¡til
- `qwen2.5:7b-instruct` - Boa performance geral
- `phi3:3.8b-instruct` - Equilibrado entre velocidade e qualidade

## ğŸ› ï¸ **Comandos DisponÃ­veis**

### **InicializaÃ§Ã£o e ConfiguraÃ§Ã£o**
```bash
llm init                    # Inicializa projeto com modelo automÃ¡tico
llm set-default-model       # Define modelo padrÃ£o global
llm change-model           # Troca modelo do projeto atual
```

### **Chat e InteraÃ§Ã£o**
```bash
llm chat                   # Inicia modo conversacional
llm list-models           # Lista modelos disponÃ­veis
```

### **Comandos do Chat** (dentro do `llm chat`)
```bash
/help                     # Mostra ajuda sobre comandos
/change-model <model>     # Troca modelo durante a sessÃ£o
/status                   # Mostra status da sessÃ£o atual
/clear                    # Limpa histÃ³rico da conversa
/save [nome]              # Salva conversa atual
/load <nome>              # Carrega conversa salva
/context                  # Mostra contexto do projeto
/exit                     # Sai da conversa
```

## ğŸ”’ **SeguranÃ§a e Privacidade**

- **ğŸ” 100% Local**: Nenhum dado Ã© enviado para servidores externos
- **ğŸ  Ollama Local**: Modelos rodam na sua mÃ¡quina
- **ğŸ“ Projetos Privados**: ConfiguraÃ§Ãµes ficam na pasta do projeto
- **ğŸš« Sem Telemetria**: NÃ£o coletamos dados de uso

## ğŸš€ **PrÃ³ximos Passos**

1. **Instale a CLI**: `npm install -g llm-cli`
2. **Configure Ollama**: `ollama pull phi3:mini`
3. **Inicialize Projeto**: `llm init`
4. **Comece a Conversar**: `llm chat`

## ğŸ—ï¸ **Arquitetura**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLM CLI       â”‚    â”‚   MCP Server    â”‚    â”‚   Ollama        â”‚
â”‚   (Interface)   â”‚â—„â”€â”€â–ºâ”‚   (Integrado)   â”‚â—„â”€â”€â–ºâ”‚   (Local LLM)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Project       â”‚    â”‚  Conversation   â”‚    â”‚  Model         â”‚
â”‚  Manager       â”‚    â”‚  Manager        â”‚    â”‚  Manager       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¤ **ContribuiÃ§Ã£o**

ContribuiÃ§Ãµes sÃ£o bem-vindas! Este projeto Ã© desenvolvido para a comunidade.

1. **Fork** o repositÃ³rio
2. **Crie** uma branch para sua feature
3. **Commit** suas mudanÃ§as
4. **Push** para a branch
5. **Abra** um Pull Request

## ğŸ“„ **LicenÃ§a**

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

---

**Desenvolvido com â¤ï¸ para a comunidade de desenvolvedores**
