# LLM CLI

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Node.js](https://img.shields.io/badge/Node.js-18+-green.svg)](https://nodejs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.3+-blue.svg)](https://www.typescriptlang.org/)

Uma CLI inteligente e conversacional para desenvolvimento assistido por IA, distribuÃ­da via NPM e otimizada para Linux. A LLM CLI utiliza o Model Context Protocol (MCP) com JSON-RPC para se comunicar com modelos LLMs locais via Vlama, oferecendo uma experiÃªncia de desenvolvimento fluida e inteligente.

## ğŸš€ CaracterÃ­sticas Principais

### ğŸ¤– **DetecÃ§Ã£o Inteligente de Hardware**
- **AnÃ¡lise AutomÃ¡tica**: Detecta CPU, RAM, GPU e armazenamento do sistema
- **RecomendaÃ§Ãµes Personalizadas**: Sugere modelos LLMs compatÃ­veis com seu hardware
- **OtimizaÃ§Ã£o de Performance**: Configura modelos baseados nas capacidades do sistema

### ğŸ—ï¸ **OrquestraÃ§Ã£o de Modelos LLMs**
- **Gerenciamento AutomÃ¡tico**: Inicializa e para modelos automaticamente por projeto
- **IntegraÃ§Ã£o Vlama**: Suporte nativo para modelos locais via Vlama
- **MCP Protocol**: ComunicaÃ§Ã£o padronizada via Model Context Protocol

### ğŸ“ **InicializaÃ§Ã£o Inteligente de Projetos**
- **DetecÃ§Ã£o AutomÃ¡tica**: Identifica linguagem e framework automaticamente
- **ConfiguraÃ§Ã£o Contextual**: Armazena contexto do projeto em Markdown
- **Modelo por Projeto**: ConfiguraÃ§Ã£o especÃ­fica de modelo para cada projeto

### ğŸ”§ **ManipulaÃ§Ã£o Inteligente de Arquivos**
- **EdiÃ§Ã£o Contextual**: Modifica arquivos preservando contexto existente
- **Sistema de Backups**: Cria backups automÃ¡ticos antes de alteraÃ§Ãµes
- **HistÃ³rico de MudanÃ§as**: Rastreia todas as alteraÃ§Ãµes com rollback

### ğŸ’¬ **Interface Conversacional Natural**
- **Comandos Intuitivos**: InteraÃ§Ã£o em linguagem natural
- **Comandos Especiais**: Sistema de comandos `/` para funcionalidades avanÃ§adas
- **Contexto Persistente**: MantÃ©m contexto entre sessÃµes

### ğŸ”„ **Funcionalidades de Desenvolvimento**
- **CriaÃ§Ã£o de Funcionalidades**: Gera testes, mÃ³dulos e componentes automaticamente
- **EdiÃ§Ã£o Inteligente**: Modifica cÃ³digo baseado em instruÃ§Ãµes em linguagem natural
- **Rollback Seguro**: Desfaz alteraÃ§Ãµes com sistema de backup integrado

## ğŸ› ï¸ InstalaÃ§Ã£o

### PrÃ©-requisitos
- **Node.js**: VersÃ£o 18 ou superior
- **Linux**: Sistema operacional Linux (Ubuntu, Debian, CentOS, etc.)
- **Vlama**: Gerenciador de modelos LLMs locais

### InstalaÃ§Ã£o Global
```bash
npm install -g llm-cli
```

### VerificaÃ§Ã£o da InstalaÃ§Ã£o
```bash
llm --version
```

## ğŸ¯ Primeiros Passos

### 1. **Detectar Hardware e Configurar**
```bash
# Detectar hardware e obter recomendaÃ§Ãµes de modelos
llm detect-hardware

# Definir modelo padrÃ£o baseado nas recomendaÃ§Ãµes
llm set-default-model phi-3:3.8b-instruct
```

### 2. **Inicializar Projeto**
```bash
# Navegar para a pasta do projeto
cd meu-projeto

# Inicializar projeto (detecta linguagem automaticamente)
llm init

# Ou especificar modelo especÃ­fico
llm init -m deepseek-coder:6.7b-instruct
```

### 3. **Modo Conversacional**
```bash
# Iniciar chat com IA
llm chat

# Ou especificar modelo para esta sessÃ£o
llm chat -m phi-3:3.8b-instruct
```

## ğŸ“š Comandos Principais

### **Gerenciamento de Projetos**
```bash
# Inicializar novo projeto
llm init [-m modelo] [-f]

# Ver status do projeto atual
llm status

# Trocar modelo do projeto
llm change-model -m novo-modelo
```

### **Gerenciamento de Modelos**
```bash
# Listar modelos disponÃ­veis
llm list-models

# Definir modelo padrÃ£o global
llm set-default-model nome-do-modelo

# Detectar hardware e recomendaÃ§Ãµes
llm detect-hardware
```

### **Desenvolvimento**
```bash
# Criar nova funcionalidade
llm create <tipo> <nome> [-d descriÃ§Ã£o]

# Editar arquivo existente
llm edit <arquivo> <instruÃ§Ã£o>

# Desfazer alteraÃ§Ãµes
llm rollback [-n nÃºmero]
```

### **Modo Conversacional**
```bash
# Iniciar chat
llm chat [-m modelo]

# Comandos disponÃ­veis no chat:
/help              # Mostra ajuda
/change-model      # Troca modelo ativo
/status            # Status da sessÃ£o
/clear             # Limpa histÃ³rico
/save [nome]       # Salva conversa
/load <nome>       # Carrega conversa
/context           # Mostra contexto do projeto
/exit              # Sai do chat
```

## ğŸ”§ ConfiguraÃ§Ã£o

### **Arquivo de ConfiguraÃ§Ã£o Global**
```bash
# LocalizaÃ§Ã£o: ~/.llm-cli/config.json
{
  "defaultModel": "phi-3:3.8b-instruct",
  "mcpConfig": {
    "serverUrl": "http://localhost",
    "port": 8000,
    "protocol": "http",
    "timeout": 30000
  },
  "vlamaConfig": {
    "modelsPath": "~/.local/share/ollama/models",
    "serverUrl": "http://localhost",
    "port": 11434,
    "maxMemory": 8192,
    "gpuEnabled": false
  }
}
```

### **ConfiguraÃ§Ã£o por Projeto**
```bash
# LocalizaÃ§Ã£o: .llm-cli/project.json
{
  "path": "/caminho/do/projeto",
  "name": "meu-projeto",
  "language": "TypeScript",
  "framework": "React",
  "model": "deepseek-coder:6.7b-instruct",
  "createdAt": "2024-01-01T00:00:00.000Z"
}
```

## ğŸ¨ Exemplos de Uso

### **Criar Testes AutomÃ¡ticos**
```bash
# No modo conversacional
llm chat

# Solicitar criaÃ§Ã£o de testes
> Crie testes unitÃ¡rios para o componente UserProfile

# A CLI irÃ¡:
# 1. Analisar o cÃ³digo existente
# 2. Gerar testes apropriados
# 3. Pedir confirmaÃ§Ã£o antes de aplicar
# 4. Criar backups automÃ¡ticos
```

### **Refatorar CÃ³digo**
```bash
# Editar arquivo com instruÃ§Ã£o especÃ­fica
llm edit src/components/UserProfile.tsx "Converta para hooks funcionais e adicione TypeScript strict"

# A CLI irÃ¡:
# 1. Ler o arquivo atual
# 2. Gerar cÃ³digo refatorado
# 3. Aplicar mudanÃ§as com backup
# 4. Registrar no histÃ³rico
```

### **Criar Nova Funcionalidade**
```bash
# Criar mÃ³dulo de autenticaÃ§Ã£o
llm create module auth -d "Sistema de autenticaÃ§Ã£o com JWT e refresh tokens"

# A CLI irÃ¡:
# 1. Analisar estrutura do projeto
# 2. Gerar arquivos necessÃ¡rios
# 3. Configurar dependÃªncias
# 4. Criar documentaÃ§Ã£o
```

## ğŸ—ï¸ Arquitetura

### **Componentes Principais**
- **LLMCLI**: Orquestrador principal da aplicaÃ§Ã£o
- **ProjectManager**: Gerenciamento de projetos e detecÃ§Ã£o de linguagens
- **ModelManager**: OrquestraÃ§Ã£o de modelos LLMs
- **ConversationManager**: Interface conversacional e comandos
- **FileManager**: ManipulaÃ§Ã£o de arquivos e sistema de backup
- **MCPClient**: Cliente para comunicaÃ§Ã£o MCP
- **VlamaManager**: Gerenciamento de modelos locais

### **Fluxo de Trabalho**
```
UsuÃ¡rio â†’ CLI â†’ ModelManager â†’ MCPClient â†’ Modelo LLM
                â†“
            FileManager â†’ Aplicar MudanÃ§as
                â†“
            ProjectManager â†’ Atualizar Contexto
```

## ğŸ”’ SeguranÃ§a e Backup

### **Sistema de Backups**
- **Backups AutomÃ¡ticos**: Criados antes de cada alteraÃ§Ã£o
- **HistÃ³rico de MudanÃ§as**: Rastreamento completo de alteraÃ§Ãµes
- **Rollback Seguro**: Desfaz alteraÃ§Ãµes com seguranÃ§a

### **Isolamento de Projetos**
- **ConfiguraÃ§Ã£o por Projeto**: Cada projeto tem suas configuraÃ§Ãµes
- **Contexto Isolado**: Contexto especÃ­fico para cada projeto
- **Modelos Independentes**: ConfiguraÃ§Ã£o de modelo por projeto

## ğŸ§ª Testes

### **Executar Testes**
```bash
# Instalar dependÃªncias
npm install

# Executar testes
npm test

# Testes em modo watch
npm run test:watch

# Cobertura de cÃ³digo
npm run test -- --coverage
```

### **Estrutura de Testes**
```
src/__tests__/
â”œâ”€â”€ setup.ts              # ConfiguraÃ§Ã£o global
â”œâ”€â”€ LLMCLI.test.ts        # Testes da classe principal
â”œâ”€â”€ core/                 # Testes dos componentes principais
â”œâ”€â”€ utils/                # Testes das utilidades
â”œâ”€â”€ mcp/                  # Testes do cliente MCP
â””â”€â”€ vlama/                # Testes do gerenciador Vlama
```

## ğŸš€ Desenvolvimento

### **Estrutura do Projeto**
```
src/
â”œâ”€â”€ core/                 # Componentes principais
â”‚   â”œâ”€â”€ LLMCLI.ts        # Classe principal
â”‚   â”œâ”€â”€ ProjectManager.ts # Gerenciador de projetos
â”‚   â”œâ”€â”€ ModelManager.ts   # Gerenciador de modelos
â”‚   â”œâ”€â”€ ConversationManager.ts # Gerenciador de conversas
â”‚   â””â”€â”€ FileManager.ts    # Gerenciador de arquivos
â”œâ”€â”€ mcp/                  # Cliente MCP
â”‚   â””â”€â”€ MCPClient.ts      # ImplementaÃ§Ã£o MCP
â”œâ”€â”€ vlama/                # Gerenciador Vlama
â”‚   â””â”€â”€ VlamaManager.ts   # IntegraÃ§Ã£o Vlama
â”œâ”€â”€ utils/                # UtilitÃ¡rios
â”‚   â”œâ”€â”€ Logger.ts         # Sistema de logging
â”‚   â”œâ”€â”€ ConfigManager.ts  # Gerenciador de configuraÃ§Ã£o
â”‚   â”œâ”€â”€ HardwareDetector.ts # Detector de hardware
â”‚   â””â”€â”€ LanguageDetector.ts # Detector de linguagens
â”œâ”€â”€ types/                # DefiniÃ§Ãµes de tipos
â”‚   â””â”€â”€ index.ts          # Tipos principais
â””â”€â”€ index.ts              # Ponto de entrada
```

### **Scripts DisponÃ­veis**
```bash
# Desenvolvimento
npm run dev              # Executar em modo desenvolvimento
npm run build            # Compilar TypeScript
npm run start            # Executar versÃ£o compilada

# Qualidade de CÃ³digo
npm run lint             # Verificar cÃ³digo
npm run lint:fix         # Corrigir problemas automaticamente

# Testes
npm run test             # Executar testes
npm run test:watch       # Testes em modo watch
npm run test:coverage    # Cobertura de testes

# Build e Deploy
npm run clean            # Limpar build
npm run prepublishOnly   # Build antes de publicar
```

## ğŸ¤ Contribuindo

### **Como Contribuir**
1. **Fork** o repositÃ³rio
2. **Clone** seu fork localmente
3. **Crie** uma branch para sua feature
4. **Desenvolva** e **teste** suas mudanÃ§as
5. **Commit** com mensagens descritivas
6. **Push** para sua branch
7. **Abra** um Pull Request

### **PadrÃµes de CÃ³digo**
- **TypeScript**: CÃ³digo tipado e estruturado
- **ESLint**: Linting automÃ¡tico
- **Prettier**: FormataÃ§Ã£o de cÃ³digo
- **Jest**: Testes unitÃ¡rios
- **Conventional Commits**: PadrÃ£o de commits

### **Checklist de Pull Request**
- [ ] CÃ³digo compila sem erros
- [ ] Testes passam
- [ ] Cobertura de testes mantida
- [ ] DocumentaÃ§Ã£o atualizada
- [ ] Linting passa
- [ ] Commits seguem padrÃ£o

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a **MIT License** - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ™ Agradecimentos

- **Vlama**: Gerenciamento de modelos LLMs locais
- **MCP**: Model Context Protocol para comunicaÃ§Ã£o padronizada
- **Comunidade Open Source**: ContribuiÃ§Ãµes e feedback

## ğŸ“ Suporte

### **Canais de Suporte**
- **Issues**: [GitHub Issues](https://github.com/llm-cli/llm-cli/issues)
- **Discussions**: [GitHub Discussions](https://github.com/llm-cli/llm-cli/discussions)
- **DocumentaÃ§Ã£o**: [Wiki do Projeto](https://github.com/llm-cli/llm-cli/wiki)

### **Problemas Comuns**
- **Modelo nÃ£o encontrado**: Verifique se o Vlama estÃ¡ rodando
- **Erro de conexÃ£o MCP**: Verifique configuraÃ§Ã£o do servidor MCP
- **Hardware nÃ£o detectado**: Execute `llm detect-hardware` para diagnÃ³stico

---

**LLM CLI** - Transformando desenvolvimento com IA conversacional ğŸš€

*Desenvolvido com â¤ï¸ pela comunidade open source*
