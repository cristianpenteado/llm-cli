# ğŸš€ LLM CLI - Agente de IA para Modelos Locais

> **CLI inteligente que funciona como um agente de IA no terminal, integrado com modelos LLMs locais via Ollama**

## ğŸ¯ **Proposta da CLI**

A **LLM CLI** Ã© uma ferramenta de linha de comando que transforma seu terminal em um assistente de IA inteligente. Diferente de outras CLIs que dependem de APIs externas, ela funciona **100% localmente** usando modelos LLMs rodando na sua mÃ¡quina via Ollama.

### ğŸŒŸ **Principais CaracterÃ­sticas**

- **ğŸ¤– Agente de IA Local**: Funciona offline com modelos rodando na sua mÃ¡quina
- **ğŸš€ InicializaÃ§Ã£o Inteligente**: Detecta automaticamente o tipo de projeto e sugere modelos compatÃ­veis
- **ğŸ’¬ Interface Conversacional**: Chat natural com comandos especiais para desenvolvimento
- **ğŸ“ Gerenciamento de Projetos**: Contexto inteligente e histÃ³rico de alteraÃ§Ãµes
- **ğŸ”„ Fallback AutomÃ¡tico**: Se o MCP falhar, usa Ollama diretamente
- **ğŸ“ IntegraÃ§Ã£o com Git**: Atualiza automaticamente `.gitignore` para incluir `.llm-cli`

## ğŸš€ **InstalaÃ§Ã£o RÃ¡pida**

### PrÃ©-requisitos
- **Node.js** 18+ e **npm**
- **Ollama** instalado e rodando
- **Linux** (Ubuntu/Debian recomendado)

### InstalaÃ§Ã£o Global
```bash
# Instalar via NPM
npm install -g llm-cli

# Verificar instalaÃ§Ã£o
llm --version
```

### Script de InstalaÃ§Ã£o AutomÃ¡tica
```bash
# Baixar e executar script de instalaÃ§Ã£o
curl -fsSL https://raw.githubusercontent.com/seu-usuario/llm-cli/main/scripts/install.sh | bash
```

## ğŸ® **Como Funciona**

### 1. **Primeira ExecuÃ§Ã£o - DetecÃ§Ã£o de Hardware**
```bash
# A CLI detecta automaticamente seu hardware
llm detect-hardware

# Recomenda modelos compatÃ­veis baseado em:
# - Processador (CPU cores, arquitetura)
# - MemÃ³ria RAM disponÃ­vel
# - GPU (se disponÃ­vel)
# - EspaÃ§o em disco
```

### 2. **InicializaÃ§Ã£o de Projeto - Modelo AutomÃ¡tico**
```bash
# Em qualquer pasta de projeto
llm init

# A CLI:
# âœ… Detecta linguagem/framework automaticamente
# ğŸ¤– Sugere modelo baseado no hardware
# ğŸ“ Cria estrutura de projeto com contexto
# ğŸ”§ Configura modelo padrÃ£o
# ğŸ“ Atualiza .gitignore automaticamente
```

### 3. **Chat Inteligente com Fallback**
```bash
# Iniciar conversa
llm chat

# Se o projeto nÃ£o estiver inicializado:
# âš ï¸ "Projeto nÃ£o inicializado! Deseja inicializar agora? (S/n)"
# ğŸš€ Inicializa automaticamente se confirmado
# ğŸ’¬ Inicia chat com modelo configurado

# Fallback automÃ¡tico:
# ğŸ”Œ Tenta conectar via MCP integrado
# âš ï¸ Se falhar, usa Ollama diretamente
# âœ… Chat funciona independente do protocolo
```

## ğŸ› ï¸ **Comandos Principais**

### **InicializaÃ§Ã£o e ConfiguraÃ§Ã£o**
```bash
llm init                    # Inicializa projeto com modelo automÃ¡tico
llm detect-hardware         # Detecta hardware e recomenda modelos
llm set-default-model       # Define modelo padrÃ£o global
llm change-model           # Troca modelo do projeto atual
```

### **Desenvolvimento Assistido**
```bash
llm chat                   # Chat conversacional com IA
llm create <tipo> <nome>   # Cria funcionalidades via IA
llm edit <arquivo> <inst>  # Edita arquivos com instruÃ§Ãµes
llm status                 # Status do projeto e modelo
```

### **Gerenciamento de Modelos**
```bash
llm list-models            # Lista modelos disponÃ­veis
llm download <modelo>      # Baixa modelo via Ollama
llm remove <modelo>        # Remove modelo local
```

## ğŸ”§ **Arquitetura Inteligente**

### **Sistema de Fallback**
```
UsuÃ¡rio â†’ LLM CLI â†’ MCP Integrado â†’ Ollama
                â†“ (se falhar)
            Ollama Direto â†’ Modelo Local
```

### **MÃ³dulos Principais**
- **ğŸ¤– ModelManager**: Orquestra modelos com fallback automÃ¡tico
- **ğŸ“ ProjectManager**: Gerenciamento inteligente de projetos
- **ğŸ’¬ ConversationManager**: Interface conversacional natural
- **ğŸ”Œ MCPClient**: Cliente MCP com servidor integrado
- **ğŸ“ FileManager**: OperaÃ§Ãµes de arquivo com histÃ³rico

## ğŸ¯ **Casos de Uso**

### **Desenvolvedor Iniciando Novo Projeto**
```bash
cd meu-projeto-nodejs
llm init                    # Detecta Node.js, sugere modelo leve
llm chat                    # "Crie um servidor Express bÃ¡sico"
```

### **Desenvolvedor em Projeto Existente**
```bash
cd projeto-python
llm chat                    # "Adicione validaÃ§Ã£o de dados na funÃ§Ã£o X"
```

### **Equipe de Desenvolvimento**
```bash
llm init                    # Configura projeto para toda equipe
llm set-default-model       # Define modelo padrÃ£o da equipe
```

## âš¡ **Modelos Recomendados por Hardware**

### **Hardware BÃ¡sico (4GB RAM, CPU 2 cores)**
- `phi3:mini` - 3.8B parÃ¢metros, rÃ¡pido e eficiente
- `gemma2:2b` - 2B parÃ¢metros, muito leve

### **Hardware MÃ©dio (8GB RAM, CPU 4 cores)**
- `deepseek-coder:6.7b-instruct` - Excelente para cÃ³digo
- `phi3:3.8b-instruct` - Equilibrado entre velocidade e qualidade

### **Hardware AvanÃ§ado (16GB+ RAM, GPU)**
- `llama3.1:8b-instruct` - Qualidade superior
- `mistral:7b-instruct` - Muito versÃ¡til

## ğŸ”’ **SeguranÃ§a e Privacidade**

- **100% Local**: Nenhum dado sai da sua mÃ¡quina
- **Sem APIs Externas**: Funciona offline
- **Controle Total**: VocÃª escolhe os modelos e configuraÃ§Ãµes
- **HistÃ³rico Local**: Conversas ficam na sua mÃ¡quina

## ğŸš€ **PrÃ³ximos Passos**

1. **Instale a CLI**: `npm install -g llm-cli`
2. **Configure Ollama**: `ollama pull phi3:mini`
3. **Inicialize Projeto**: `llm init`
4. **Comece a Conversar**: `llm chat`

## ğŸ¤ **Contribuindo**

ContribuiÃ§Ãµes sÃ£o bem-vindas! A CLI Ã© open-source e aceita:
- ğŸ› Reportes de bugs
- ğŸ’¡ SugestÃµes de funcionalidades
- ğŸ”§ Pull requests
- ğŸ“š Melhorias na documentaÃ§Ã£o

## ğŸ“„ **LicenÃ§a**

MIT License - Veja [LICENSE](LICENSE) para detalhes.

## ğŸ™ **Agradecimentos**

- **Ollama** pela infraestrutura de modelos locais
- **Model Context Protocol** pelo padrÃ£o de comunicaÃ§Ã£o
- **Comunidade open-source** por inspiraÃ§Ã£o e suporte

---

**ğŸ‰ Transforme seu terminal em um assistente de IA inteligente e local!**
